{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b4376f-f7a5-4aab-86c8-f25ddef37b82",
   "metadata": {},
   "source": [
    "## VLBart PyReft Integration\n",
    "This is my preliminary try on integrating VLBart with PyReft.\n",
    "### Instructions\n",
    "1. Use Pyvene's peterwz-llava branch and PyReft's peterwz-llava branch.\n",
    "2. Head to pyreft/examples/vlbart/DoRA/image_video_text_understanding, and install packages with the same version as the requirements.txt there. Note that DoRA requires a much less transformers version.\n",
    "3. Download dataset according to the instructions in pyreft/examples/vlbart/DoRA/image_video_text_understanding/README.md, specifically, go to the google drive link and download processed CLIP features. Put it in pyreft/examples/vlbart/DoRA/datasets/ In this notebook we only process on VQA features.\n",
    "4. In image_video_text_understanding/download_backbones.py, change the cache directory to your directory storing the models.\n",
    "5. Try run image_video_text_understanding/VL-T5/scripts/image/dora.sh to see if your DoRA (VLBart model) is installed successfully.\n",
    "6. Run this notebook.\n",
    "### Known Issues\n",
    "1. The model generation results may be incorrect, as you can see from the generation experiments below. Intervention locations are all untested.\n",
    "2. I removed the \"+1\" padding when performing PyReft interventions (you can see that \"padding\" is \"none\" instead of \"left\")\n",
    "3. The training is fast in first few steps, then become very slow. I suspect that is related to the data loading cache behavior. Batching the dataset loading process, instead of the lazy data loading we are using now with ReftDataloaderDataset, may be a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646cf6d8-cb3e-49c8-a087-61c9b162a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'DoRA/image_video_text_understanding/VL-T5/src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8704192a-b100-4028-a320-c94079566f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vqa_clip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dbad52-fd2b-41e3-b290-7f511452c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_args = {'RefCOCO_BUTD': False,\n",
    " 'RefCOCO_GT': False,\n",
    " 'adam_beta1': 0.9,\n",
    " 'adam_beta2': 0.999,\n",
    " 'adam_eps': 1e-06,\n",
    " 'add_adapter_cross_attn': True,\n",
    " 'add_layer_norm_after_adapter': False,\n",
    " 'add_layer_norm_before_adapter': False,\n",
    " 'additional_visual_embedding_layers': 0,\n",
    " 'answer_normalize': False,\n",
    " 'backbone': 'facebook/bart-base',\n",
    " 'batch_size': 512,\n",
    " 'caption_cocoonly': True,\n",
    " 'caption_only': False,\n",
    " 'classifier': False,\n",
    " 'clip_grad_norm': 5.0,\n",
    " 'cls_task': 'tinyimagenet',\n",
    " 'coco_only': False,\n",
    " 'comment': '',\n",
    " 'decoder_prompt_len': 0,\n",
    " 'deepspeed': None,\n",
    " 'distributed': False,\n",
    " 'do_lower_case': False,\n",
    " 'dora_simple': False,\n",
    " 'downsample': True,\n",
    " 'dropout': 0.1,\n",
    " 'dry': False,\n",
    " 'efficient_unique_hyper_net': False,\n",
    " 'encoder_prompt_len': 0,\n",
    " 'epochs': 20,\n",
    " 'expand_vis_embedding': False,\n",
    " 'factorized_phm': True,\n",
    " 'feat_dim': 2048,\n",
    " 'feature_type': 'RN101',\n",
    " 'fp16': False,\n",
    " 'freeze_bn_statistics': False,\n",
    " 'freeze_ln_statistics': False,\n",
    " 'from_scratch': False,\n",
    " 'full_determinism': False,\n",
    " 'gen_max_length': 20,\n",
    " 'gpu': 0,\n",
    " 'gradient_accumulation_steps': 1,\n",
    " 'ground_upsample': 1,\n",
    " 'ground_weight': 1,\n",
    " 'hypercomplex_division': 4,\n",
    " 'image_size': '(224,224)',\n",
    " 'individual_vis_layer_norm': True,\n",
    " 'itm_cocoonly': True,\n",
    " 'lambda_z': 0.001,\n",
    " 'load': None,\n",
    " 'load_lxmert_qa': None,\n",
    " 'local_rank': 0,\n",
    " 'log_train_accuracy': False,\n",
    " 'lora_alpha': 32,\n",
    " 'lora_dim': 128,\n",
    " 'lora_settings': True,\n",
    " 'losses': 'lm,obj,attr,feat',\n",
    " 'low_rank_rank': 1,\n",
    " 'lr': 0.001,\n",
    " 'max_n_boxes': 36,\n",
    " 'max_text_length': 20,\n",
    " 'mid_dim': 768,\n",
    " 'multiGPU': True,\n",
    " 'multitask_sampling': 'roundrobin',\n",
    " 'n_boxes': 36,\n",
    " 'n_ground': 1,\n",
    " 'n_image_tokens': 4,\n",
    " 'no_prefix': False,\n",
    " 'num_beams': 5,\n",
    " 'num_workers': 4,\n",
    " 'obj_mask_rate': 0.15,\n",
    " 'oneddownsample': False,\n",
    " 'optim': 'adamw',\n",
    " 'optimizer': 'adamw',\n",
    " 'oscar_tags': False,\n",
    " 'output': 'snap/VLBart_multitask/tune+lr1e-3_plzplz2',\n",
    " 'phm_init_range': 0.01,\n",
    " 'phm_rank': 1,\n",
    " 'pos_dim': 4,\n",
    " 'post_prompt': '',\n",
    " 'prefix': None,\n",
    " 'project_name': 'RN101_LMsingle_dora_128_bs300_image224_lora_settings',\n",
    " 'projected_task_embedding_dim': -1,\n",
    " 'prompt': 'vqa: ',\n",
    " 'raw_label': False,\n",
    " 'reduction_factor': 16,\n",
    " 'remove_bn_vis_adapter': False,\n",
    " 'run_name': 'tune+lr1e-3_plzplz2',\n",
    " 'seed': 9595,\n",
    " 'share_down_sampler': False,\n",
    " 'share_up_sampler': False,\n",
    " 'share_vis_lang_layer_norm': False,\n",
    " 'shared_phm_rule': True,\n",
    " 'shared_phm_rule_over_tasks': False,\n",
    " 'shuffle_boxes': False,\n",
    " 'single_vqa_prefix': False,\n",
    " 'sparse_sample': False,\n",
    " 'submit': False,\n",
    " 'tasks': 'vqa',\n",
    " 'test': None,\n",
    " 'test_answerable': False,\n",
    " 'test_only': False,\n",
    " 'testing': False,\n",
    " 'tokenizer': None,\n",
    " 'track_z': False,\n",
    " 'train': 'train',\n",
    " 'train_topk': -1,\n",
    " 'unfreeze_batch_norms': False,\n",
    " 'unfreeze_bias': True,\n",
    " 'unfreeze_decoder_layer_norms': False,\n",
    " 'unfreeze_encoder_layer_norms': False,\n",
    " 'unfreeze_language_model': False,\n",
    " 'unfreeze_layer_norms': True,\n",
    " 'unfreeze_lm_head': False,\n",
    " 'unfreeze_vis_encoder': False,\n",
    " 'unfreeze_vis_last_layer': False,\n",
    " 'unique_hyper_net': False,\n",
    " 'use_adam_for_visual': False,\n",
    " 'use_adapter': False,\n",
    " 'use_attn_prefix': False,\n",
    " 'use_compacter': False,\n",
    " 'use_data_augmentation': False,\n",
    " 'use_dora': True,\n",
    " 'use_hyperformer': False,\n",
    " 'use_lm_head_adapter': False,\n",
    " 'use_lora': False,\n",
    " 'use_lradapter': False,\n",
    " 'use_separate_optimizer_for_visual': False,\n",
    " 'use_single_adapter': False,\n",
    " 'use_single_lora': False,\n",
    " 'use_single_prompt': False,\n",
    " 'use_tasks_prompts': True,\n",
    " 'use_vis_adapter': False,\n",
    " 'use_vis_layer_norm': True,\n",
    " 'use_vis_order_embedding': True,\n",
    " 'use_vision': True,\n",
    " 'valid': 'valid',\n",
    " 'valid_batch_size': 512,\n",
    " 'valid_topk': -1,\n",
    " 'vis_adapter_type': 'middle-bottleneck',\n",
    " 'vis_lr': 0.0001,\n",
    " 'vis_pointer': False,\n",
    " 'vis_pooling_output': False,\n",
    " 'vis_reduction_factor': 2,\n",
    " 'vis_use_transformer': False,\n",
    " 'vis_weight_decay': 0.01,\n",
    " 'warmup_ratio': 0.1,\n",
    " 'weight_decay': 0.01,\n",
    " 'word_mask_rate': 0.15,\n",
    " 'world_size': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "849d4858-ed49-435b-af99-558330dc0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace(**vqa_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956ea1f7-b855-4159-9b84-a3819844b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 605102 data from split(s) karpathy_train.\n",
      "# Answers: 3129\n",
      "Data sources:  ['karpathy_train']\n",
      "Loaded 605102 data from karpathy_train\n",
      "# all sentences: 605102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loaders = []\n",
    "vqa_train_loader = vqa_clip_data.get_loader(\n",
    "    args,\n",
    "    split='karpathy_train', mode='train', batch_size=args.batch_size,\n",
    "    distributed=args.distributed, gpu=0,\n",
    "    workers=args.num_workers,\n",
    "    topk=args.train_topk,\n",
    ")\n",
    "train_loaders.append(vqa_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4dfab9b-e1a0-4765-9676-dc9001117163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyreft.dataset import ReftDataset, ReftDataloaderDataset\n",
    "from pyreft import (\n",
    "    ReftTrainerForCausalLM, \n",
    "    ReftDataCollator,\n",
    "    LoreftIntervention,\n",
    "    TaskType,\n",
    "    ReftConfig,\n",
    "    get_reft_model,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b07f38-308c-40ed-aab2-1c98eb9e041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLBartDataset(ReftDataloaderDataset):\n",
    "    \"\"\"\n",
    "    A ReftClassificationDataset only contains a single text field\n",
    "    that we tokenize, intervene on a prefix + suffix of, and\n",
    "    compute subspace settings for. This is intended for classification\n",
    "    tasks.\n",
    "\n",
    "    Remember to pass in the input_field and label_field as kwargs.\n",
    "    \"\"\"\n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load the dataset (or a portion of it) from HF or a local file.\"\"\"\n",
    "\n",
    "        self.task_dataset = self.dataloader.dataset\n",
    "        self.collate_fn = self.task_dataset.collate_fn\n",
    "        self.pad_mode = \"none\"\n",
    "\n",
    "        # select n random examples if specificed\n",
    "        if self.max_n_example is not None:\n",
    "            self.task_dataset = torch.utils.data.Subset(self.task_dataset, list(range(self.max_n_example)))\n",
    "\n",
    "        # save raw_dataset pointer for access raw strings\n",
    "        self.raw_dataset = self.task_dataset if self.data_split != \"train\" else None\n",
    "        return self.task_dataset\n",
    "\n",
    "    def preprocess(self, kwargs):\n",
    "        self.input_field = \"input_ids\"\n",
    "        self.label_field = \"target_ids\"\n",
    "\n",
    "    def tokenize(self, data_item):\n",
    "        result = {**data_item}\n",
    "        # result[self.label_field] = \n",
    "        \n",
    "        last_position = len(data_item[self.input_field]) - 1\n",
    "            \n",
    "        return result, last_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da558d5-3d05-4178-95b3-bf6c5be3207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, TrainingArguments\n",
    "tokenizer = BartTokenizer.from_pretrained(\n",
    "    args.backbone,\n",
    "    max_length=args.max_text_length,\n",
    "    do_lower_case=args.do_lower_case\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9996cb2-b319-4e3a-a43a-b13003c4efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,2,3]\n",
    "position = \"f1+l1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2168b173-b7ec-492b-aaf5-dfb115011505",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VLBartDataset(\n",
    "    \"vqa\", \n",
    "    tokenizer, data_split=\"train\", \n",
    "    dataloader=vqa_train_loader,\n",
    "    max_n_example=1000,\n",
    "    **{\"num_interventions\": len(layers), \"position\": position, \n",
    "       \"share_weights\": True, \"test_split\": \"validation\"}\n",
    ")\n",
    "eval_dataset = VLBartDataset(\n",
    "    \"vqa\", \n",
    "    tokenizer, data_split=\"val\", \n",
    "    dataloader=vqa_train_loader,\n",
    "    max_n_example=1000,\n",
    "    **{\"num_interventions\": len(layers), \"position\": position, \n",
    "       \"share_weights\": True, \"test_split\": \"validation\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b904dc-61c7-45bb-a3d8-f60bdfaf3e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model at GPU 0\n",
      "Model Launching at GPU 0\n",
      "model.encoder.visual_embedding.feat_embedding.0.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.0.bias is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.bias is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.weight is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.bias is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.weight is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.bias is trainable...\n",
      "model.encoder.visual_embedding.img_order_embedding.weight is trainable...\n",
      "apply dora tuning\n",
      "model.encoder.layers.0.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.0.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.0.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.0.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.1.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.1.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.1.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.2.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.2.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.2.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.3.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.3.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.3.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.4.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.4.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.4.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.5.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.5.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.5.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layernorm_embedding.bias is trainable...(768)\n",
      "model.encoder.visual_embedding.feat_embedding.0.bias is trainable...(768)\n",
      "model.encoder.visual_embedding.feat_embedding.1.bias is trainable...(768)\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.bias is trainable...(768)\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.0.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.0.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.0.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.1.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.1.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.1.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.2.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.2.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.2.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.3.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.3.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.3.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.4.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.4.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.4.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.5.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.5.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.5.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layernorm_embedding.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.0.final_layer_norm is trainable...\n",
      "model.encoder.layers.1.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.1.final_layer_norm is trainable...\n",
      "model.encoder.layers.2.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.2.final_layer_norm is trainable...\n",
      "model.encoder.layers.3.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.3.final_layer_norm is trainable...\n",
      "model.encoder.layers.4.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.4.final_layer_norm is trainable...\n",
      "model.encoder.layers.5.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.5.final_layer_norm is trainable...\n",
      "model.encoder.layernorm_embedding is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1 is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1 is trainable...\n",
      "model.decoder.layers.0.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.0.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.0.final_layer_norm is trainable...\n",
      "model.decoder.layers.1.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.1.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.1.final_layer_norm is trainable...\n",
      "model.decoder.layers.2.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.2.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.2.final_layer_norm is trainable...\n",
      "model.decoder.layers.3.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.3.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.3.final_layer_norm is trainable...\n",
      "model.decoder.layers.4.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.4.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.4.final_layer_norm is trainable...\n",
      "model.decoder.layers.5.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.5.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.5.final_layer_norm is trainable...\n",
      "model.decoder.layernorm_embedding is trainable...\n",
      "VLBartMultiTask(\n",
      "  (model): VLBartModel(\n",
      "    (shared): Embedding(50465, 768)\n",
      "    (encoder): JointEncoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (q_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (visual_embedding): VisualEmbedding(\n",
      "        (feat_embedding): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (absolute_vis_pos_embedding): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (obj_order_embedding): Embedding(50465, 768)\n",
      "        (img_order_embedding): Embedding(2, 768)\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "      )\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (q_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (q_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50465, bias=False)\n",
      ")\n",
      "VLBartMultiTask(\n",
      "  (model): VLBartModel(\n",
      "    (shared): Embedding(50465, 768)\n",
      "    (encoder): JointEncoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (q_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (visual_embedding): VisualEmbedding(\n",
      "        (feat_embedding): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (absolute_vis_pos_embedding): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (obj_order_embedding): Embedding(50465, 768)\n",
      "        (img_order_embedding): Embedding(2, 768)\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "      )\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (q_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (q_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50465, bias=False)\n",
      ")\n",
      "Trainable param percentage: 5.96% (8838912/148262400)\n",
      "Building Optimizer\n",
      "Batch per epoch: 1182\n",
      "Total Iters: 23640\n",
      "Warmup ratio: 0.1\n",
      "Warm up Iters: 2364\n",
      "It took 0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from multitask import Trainer\n",
    "trainer = Trainer(args, vqa_train_loader, None, None, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d52018fc-388d-45e3-a842-c7b953a6eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c11518fa-da9f-47d1-993f-e0fa27810f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartConfig {\n",
      "  \"RefCOCO_BUTD\": false,\n",
      "  \"RefCOCO_GT\": false,\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"adam_beta1\": 0.9,\n",
      "  \"adam_beta2\": 0.999,\n",
      "  \"adam_eps\": 1e-06,\n",
      "  \"adapter_config\": null,\n",
      "  \"add_adapter_cross_attn\": true,\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"add_layer_norm_after_adapter\": false,\n",
      "  \"add_layer_norm_before_adapter\": false,\n",
      "  \"additional_visual_embedding_layers\": 0,\n",
      "  \"answer_normalize\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"backbone\": \"facebook/bart-base\",\n",
      "  \"batch_size\": 512,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"caption_cocoonly\": true,\n",
      "  \"caption_only\": false,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier\": false,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"clip_grad_norm\": 5.0,\n",
      "  \"cls_task\": \"tinyimagenet\",\n",
      "  \"coco_only\": false,\n",
      "  \"comment\": \"\",\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_prompt_config\": null,\n",
      "  \"decoder_prompt_len\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"deepspeed\": null,\n",
      "  \"default_obj_order_ids\": [\n",
      "    50464,\n",
      "    50463,\n",
      "    50462,\n",
      "    50461,\n",
      "    50460,\n",
      "    50459,\n",
      "    50458,\n",
      "    50457,\n",
      "    50456,\n",
      "    50455,\n",
      "    50454,\n",
      "    50453,\n",
      "    50452,\n",
      "    50451,\n",
      "    50450,\n",
      "    50449,\n",
      "    50448,\n",
      "    50447,\n",
      "    50446,\n",
      "    50445,\n",
      "    50444,\n",
      "    50443,\n",
      "    50442,\n",
      "    50441,\n",
      "    50440,\n",
      "    50439,\n",
      "    50438,\n",
      "    50437,\n",
      "    50436,\n",
      "    50435,\n",
      "    50434,\n",
      "    50433,\n",
      "    50432,\n",
      "    50431,\n",
      "    50430,\n",
      "    50429,\n",
      "    50428,\n",
      "    50427,\n",
      "    50426,\n",
      "    50425,\n",
      "    50424,\n",
      "    50423,\n",
      "    50422,\n",
      "    50421,\n",
      "    50420,\n",
      "    50419,\n",
      "    50418,\n",
      "    50417,\n",
      "    50416,\n",
      "    50415,\n",
      "    50414,\n",
      "    50413,\n",
      "    50412,\n",
      "    50411,\n",
      "    50410,\n",
      "    50409,\n",
      "    50408,\n",
      "    50407,\n",
      "    50406,\n",
      "    50405,\n",
      "    50404,\n",
      "    50403,\n",
      "    50402,\n",
      "    50401,\n",
      "    50400,\n",
      "    50399,\n",
      "    50398,\n",
      "    50397,\n",
      "    50396,\n",
      "    50395,\n",
      "    50394,\n",
      "    50393,\n",
      "    50392,\n",
      "    50391,\n",
      "    50390,\n",
      "    50389,\n",
      "    50388,\n",
      "    50387,\n",
      "    50386,\n",
      "    50385,\n",
      "    50384,\n",
      "    50383,\n",
      "    50382,\n",
      "    50381,\n",
      "    50380,\n",
      "    50379,\n",
      "    50378,\n",
      "    50377,\n",
      "    50376,\n",
      "    50375,\n",
      "    50374,\n",
      "    50373,\n",
      "    50372,\n",
      "    50371,\n",
      "    50370,\n",
      "    50369,\n",
      "    50368,\n",
      "    50367,\n",
      "    50366,\n",
      "    50365\n",
      "  ],\n",
      "  \"distributed\": false,\n",
      "  \"do_lower_case\": false,\n",
      "  \"dora_simple\": false,\n",
      "  \"downsample\": true,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"dry\": false,\n",
      "  \"early_stopping\": true,\n",
      "  \"efficient_unique_hyper_net\": false,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"encoder_prompt_config\": null,\n",
      "  \"encoder_prompt_len\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"epochs\": 20,\n",
      "  \"expand_vis_embedding\": false,\n",
      "  \"factorized_phm\": true,\n",
      "  \"feat_dim\": 2048,\n",
      "  \"feature_type\": \"RN101\",\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"fp16\": false,\n",
      "  \"freeze_bn_statistics\": false,\n",
      "  \"freeze_ln_statistics\": false,\n",
      "  \"from_scratch\": false,\n",
      "  \"full_determinism\": false,\n",
      "  \"gen_max_length\": 20,\n",
      "  \"gpu\": 0,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"ground_upsample\": 1,\n",
      "  \"ground_weight\": 1,\n",
      "  \"hypercomplex_division\": 4,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"image_size\": \"(224,224)\",\n",
      "  \"individual_vis_layer_norm\": true,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"itm_cocoonly\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"lambda_z\": 0.001,\n",
      "  \"load\": null,\n",
      "  \"load_lxmert_qa\": null,\n",
      "  \"local_rank\": 0,\n",
      "  \"log_train_accuracy\": false,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dim\": 128,\n",
      "  \"lora_settings\": true,\n",
      "  \"losses\": \"lm,obj,attr,feat\",\n",
      "  \"low_rank_rank\": 1,\n",
      "  \"lr\": 0.001,\n",
      "  \"max_n_boxes\": 36,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"max_text_length\": 20,\n",
      "  \"mid_dim\": 768,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"multiGPU\": true,\n",
      "  \"multitask_sampling\": \"roundrobin\",\n",
      "  \"n_boxes\": 36,\n",
      "  \"n_ground\": 1,\n",
      "  \"n_image_tokens\": 4,\n",
      "  \"n_images\": 2,\n",
      "  \"no_prefix\": false,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_workers\": 4,\n",
      "  \"obj_mask_rate\": 0.15,\n",
      "  \"oneddownsample\": false,\n",
      "  \"optim\": \"adamw\",\n",
      "  \"optimizer\": \"adamw\",\n",
      "  \"oscar_tags\": false,\n",
      "  \"output\": \"snap/VLBart_multitask/tune+lr1e-3_plzplz2\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"phm_init_range\": 0.01,\n",
      "  \"phm_rank\": 1,\n",
      "  \"pos_dim\": 4,\n",
      "  \"post_prompt\": \"\",\n",
      "  \"project_name\": \"RN101_LMsingle_dora_128_bs300_image224_lora_settings\",\n",
      "  \"projected_task_embedding_dim\": -1,\n",
      "  \"prompt\": \"vqa: \",\n",
      "  \"raw_label\": false,\n",
      "  \"reduction_factor\": 16,\n",
      "  \"remove_bn_vis_adapter\": false,\n",
      "  \"run_name\": \"tune+lr1e-3_plzplz2\",\n",
      "  \"scale_embedding\": false,\n",
      "  \"seed\": 9595,\n",
      "  \"share_down_sampler\": false,\n",
      "  \"share_up_sampler\": false,\n",
      "  \"share_vis_lang_layer_norm\": false,\n",
      "  \"shared_phm_rule\": true,\n",
      "  \"shared_phm_rule_over_tasks\": false,\n",
      "  \"shuffle_boxes\": false,\n",
      "  \"single_vqa_prefix\": false,\n",
      "  \"sparse_sample\": false,\n",
      "  \"submit\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"tasks\": \"vqa\",\n",
      "  \"test\": null,\n",
      "  \"test_answerable\": false,\n",
      "  \"test_only\": false,\n",
      "  \"testing\": false,\n",
      "  \"tokenizer\": \"facebook/bart-base\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"track_z\": false,\n",
      "  \"train\": \"train\",\n",
      "  \"train_topk\": -1,\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"unfreeze_batch_norms\": false,\n",
      "  \"unfreeze_bias\": true,\n",
      "  \"unfreeze_decoder_layer_norms\": false,\n",
      "  \"unfreeze_encoder_layer_norms\": false,\n",
      "  \"unfreeze_language_model\": false,\n",
      "  \"unfreeze_layer_norms\": true,\n",
      "  \"unfreeze_lm_head\": false,\n",
      "  \"unfreeze_vis_encoder\": false,\n",
      "  \"unfreeze_vis_last_layer\": false,\n",
      "  \"unique_hyper_net\": false,\n",
      "  \"use_adam_for_visual\": false,\n",
      "  \"use_adapter\": false,\n",
      "  \"use_attn_prefix\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"use_compacter\": false,\n",
      "  \"use_data_augmentation\": false,\n",
      "  \"use_dora\": true,\n",
      "  \"use_hyperformer\": false,\n",
      "  \"use_lm_head_adapter\": false,\n",
      "  \"use_lora\": false,\n",
      "  \"use_lradapter\": false,\n",
      "  \"use_separate_optimizer_for_visual\": false,\n",
      "  \"use_single_adapter\": false,\n",
      "  \"use_single_lora\": false,\n",
      "  \"use_single_prompt\": false,\n",
      "  \"use_tasks_prompts\": true,\n",
      "  \"use_vis_adapter\": false,\n",
      "  \"use_vis_layer_norm\": true,\n",
      "  \"use_vis_order_embedding\": true,\n",
      "  \"use_vision\": true,\n",
      "  \"valid\": \"valid\",\n",
      "  \"valid_batch_size\": 512,\n",
      "  \"valid_topk\": -1,\n",
      "  \"vis_adapter_type\": \"middle-bottleneck\",\n",
      "  \"vis_lr\": 0.0001,\n",
      "  \"vis_pointer\": false,\n",
      "  \"vis_pooling_output\": false,\n",
      "  \"vis_reduction_factor\": 2,\n",
      "  \"vis_use_transformer\": false,\n",
      "  \"vis_weight_decay\": 0.01,\n",
      "  \"vocab_size\": 50465,\n",
      "  \"warmup_ratio\": 0.1,\n",
      "  \"weight_decay\": 0.01,\n",
      "  \"word_mask_rate\": 0.15,\n",
      "  \"world_size\": 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64daf75f-7f3e-432d-9e33-75ea90d4cd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method VQAFineTuneDataset.collate_fn of <vqa_clip_data.VQAFineTuneDataset object at 0x7fdf5355f490>>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2903c7a-0a60-40f5-9d9c-6509dc891d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['args', 'img_id', 'vis_feats', 'boxes', 'question_id', 'sent', 'input_ids', 'input_length', 'is_topk_optimal', 'label', 'answer', 'score', 'all_answers', 'target_ids', 'target_length'])\n"
     ]
    }
   ],
   "source": [
    "print(vqa_train_loader.dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae73144-b449-41a4-b7d1-1033117717f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForSeq2Seq\n",
    "# data_collator_fn = DataCollatorForSeq2Seq(\n",
    "#     tokenizer=tokenizer,\n",
    "#     model=model,\n",
    "#     label_pad_token_id=-100,\n",
    "#     padding=\"longest\"\n",
    "# )\n",
    "import transformers\n",
    "def keep_intervention_locations(datum):\n",
    "    new_data = {}\n",
    "    new_data[\"input_ids\"] = datum[\"input_ids\"]\n",
    "    # new_data[\"labels\"] = datum[\"labels\"]\n",
    "    new_data[\"intervention_locations\"] = datum[\"intervention_locations\"]\n",
    "    new_data[\"attention_mask\"] = datum[\"attention_mask\"]\n",
    "    return new_data\n",
    "\n",
    "def custom_collate_fn(data):\n",
    "    collate_fn_1 = train_dataset.collate_fn\n",
    "    collate_fn_2 = transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    # print(data[0].keys())\n",
    "    output_1 = collate_fn_1(data)\n",
    "    # print(\"LABEL:\", output[\"labels\"])\n",
    "    custom_data = [keep_intervention_locations(item) for item in data]\n",
    "    output_2 = collate_fn_2(custom_data)\n",
    "    output = output_1\n",
    "    output[\"intervention_locations\"] = output_2[\"intervention_locations\"]\n",
    "    # output[\"id\"] = output_2[\"id\"]\n",
    "    # output[\"labels\"] = output_2[\"labels\"]\n",
    "    \n",
    "    output[\"attention_mask\"] = output_2[\"attention_mask\"]\n",
    "\n",
    "    ids = []\n",
    "    for d in data:\n",
    "        ids.append(d[\"id\"])\n",
    "    import numpy as np\n",
    "    output[\"id\"] = np.array(ids)\n",
    "    \n",
    "    output[\"logits\"] = output[\"labels\"]\n",
    "    output[\"labels\"] = output[\"target_ids\"]    \n",
    "\n",
    "    return output\n",
    "\n",
    "data_collator = ReftDataCollator(data_collator=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a497868a-120f-4abe-8bc7-bc1b9cbb264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1\n",
    "dropout=0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "668e60d3-61d0-48ca-bf94-b787879b1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [{\n",
    "    \"layer\": l, \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": rank,\n",
    "    \"intervention\": LoreftIntervention(\n",
    "        embed_dim=model.config.d_model, low_rank_dimension=rank,\n",
    "        dropout=dropout, dtype=torch.float32, act_fn=None, device=\"cuda\",\n",
    "        add_bias=True\n",
    "    )\n",
    "} for l in layers]\n",
    "task_type=TaskType.CAUSAL_LM\n",
    "\n",
    "reft_config = ReftConfig(representations=representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0271831c-7645-4ca5-845f-7b5a4caf8fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 4,611 || trainable model params: 0\n",
      "model params: 148,262,400 || trainable%: 0.0031100265475265476\n"
     ]
    }
   ],
   "source": [
    "reft_model = get_reft_model(model, reft_config)\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9673c723-0a55-4b39-b261-b0bced33e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"random\",\n",
    "    run_name=\"random\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    "    per_device_eval_batch_size=args.batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    metric_for_best_model=None,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_total_limit=1, # for GLUE, it will save 2 at max.\n",
    "    logging_steps=1,\n",
    "    learning_rate=1e-3,\n",
    "    warmup_ratio=0.05,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0,\n",
    "    report_to=\"none\",\n",
    "    use_cpu=False,\n",
    "    seed=42,\n",
    "    # until HF supports ReFT, this remains False! :)\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fce2fca-1c68-40bd-b143-a0d5267bb3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# class MyTrainer(ReftTrainerForCausalLM):\n",
    "#     def get_train_dataloader(self) -> DataLoader:\n",
    "#         return make_dataloader(self.train_dataset, self._train_batch_size, self.data_collator, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5201485-07d7-4dfc-b8ac-b64c1edeffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = ReftTrainerForCausalLM(\n",
    "    model=reft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d479808-d1a7-4343-8307-48a65eb0ac15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLBartMultiTask(\n",
      "  (model): VLBartModel(\n",
      "    (shared): Embedding(50465, 768)\n",
      "    (encoder): JointEncoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (q_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (visual_embedding): VisualEmbedding(\n",
      "        (feat_embedding): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (absolute_vis_pos_embedding): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (obj_order_embedding): Embedding(50465, 768)\n",
      "        (img_order_embedding): Embedding(2, 768)\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "      )\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (q_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (q_proj): DoraLinear(in_features=768, out_features=768, bias=True, lora_dim=128, lora_scale=1.0)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50465, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(reft_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e46e94b8-3d45-430d-91bd-10ef4484967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 0.001, 'epoch': 0.5}\n",
      "{'loss': 0.0009, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'eval_loss': 3.827400360023603e-05, 'eval_runtime': 1.6025, 'eval_samples_per_second': 624.033, 'eval_steps_per_second': 1.248, 'epoch': 1.0}\n",
      "{'train_runtime': 3.8445, 'train_samples_per_second': 260.11, 'train_steps_per_second': 0.52, 'train_loss': 0.0006809134501963854, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=0.0006809134501963854, metrics={'train_runtime': 3.8445, 'train_samples_per_second': 260.11, 'train_steps_per_second': 0.52, 'train_loss': 0.0006809134501963854, 'epoch': 1.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b39f7722-e83d-4543-811e-faa6345179b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 21512, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1bf162a-1788-48dc-9ac9-bd2bb76cb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "reft_model.model.eval()\n",
    "for k,v in reft_model.interventions.items():\n",
    "    _ = v[0].eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "835e19b8-bb8b-456e-b05c-b3e0ad97bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 0/2 [00:00<?, ?it/s]/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  0%|                                                                                                                        | 0/2 [00:16<?, ?it/s, em=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "000000000300000000:000000/000000.000000100000\n",
      "AtIStoppelisisislamisperiamsperiamisis\n",
      "I'm a professional baseball player?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "At the persons headwear?\n",
      "000000000aditalitalitaliammer's handEl000000300000000\n",
      "”” event event at event event event� event event moment event event –\n",
      "South Africa’s tennis ball or frisbee?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I (I)I (Limpitalitalitali000000300000000\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "RestRestRest RestRest Rest Rest RestRestRest of RestRest ofRest Rest of\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "”” event event at event event event� event event moment event event –\n",
      "is the business man doing in the picture?\n",
      "Prostoppet well with his suit?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "isis wearing a plain tie?\n",
      "vqa: Judging from the dress, was this taken in a Latin American country?\n",
      "I (not shown in this picture?\n",
      "AtRestRestRestitaliammermeriammeriamad000000000/000\n",
      "I/I/Em/impimpac//m/i/m\n",
      "IIIa a tattoo?\n",
      "Eladendendendisislammeriendendiendislami\n",
      "““He’s a thug”: How many tattoos can\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "I/I/Em/impimpac//m/i/m\n",
      "isputput on a dirt path?\n",
      "Eladendendendisislammeriendendiendislami\n",
      "I/I/Em/impimpac//m/i/m\n",
      "vqa: How many pieces of furniture which are used for sleeping are featured in this\n",
      "Iputputputsputputtputsunderturlsputsper\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000000 at the back?El——000000300000000 100000000\n",
      "000000000+0000001st class?000000%000000.000\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I am in this image?\n",
      "I.R.I.N.S. I’m in the foreground\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Inputputputsputputtedputputtputput(sic)put\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "”” event event at event event event� event event moment event event –\n",
      "Iam a hat?\n",
      "Eladendendendisislammeriendendiendislami\n",
      "II made a collar?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "0000000001000000c000000ad000000kundertundert\n",
      "I/I/Em/impimpac//m/i/m\n",
      "On the red carpet, on the TV, indicate?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I am the house is this?\n",
      "100000000000,000000.000000 100000000(000000)\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "vqa: What companion object to the TV can be seen in the bottom right of\n",
      "\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "tendendendelasputputputsputt.Ikendend\n",
      "\n",
      "Lamendendendislamendendiamsendendamendisend\n",
      "I’m the one with the “I am“I“\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "000000000 the picture?\n",
      "StopposedStoppurti made made made out?Stoppopposed.\n",
      "badamamamis is this?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "At the end of the event, the person have on his face?\n",
      "is in front of the giraffes?\n",
      "\n",
      "I was from a zoo?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "Proopp eating the tree?\n",
      "is standing?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "vqa: What is on the ground next to the giraffe on the right?\n",
      "RestRestRest RestRestRestoppelis /StoppeliRestResti\n",
      "Stoppoppoppeloppelis eating?\n",
      "Iurturt in the shade?\n",
      "RestRestRest ofRest of theRest of Rest of the rest of the Rest of\n",
      "is is there?\n",
      "\n",
      "I (not) in this photo?\n",
      "Eloppoppeloppel.Eloppel,EloppendendendEl\n",
      "I/I/Em/impimpac//m/i/m\n",
      "vqa: What does the word in the upper-left corner, behind the window\n",
      "I am on the boat?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is like?000000000 watercraft is that?\n",
      "Stopposedpublicispublicis.compublicisPublicis.000000000\n",
      "is this?\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I was in the vase?\n",
      "Istoppoppoppel Istoppel made out of the attack. I\n",
      "is the sun to the right or left of this flower?\n",
      "”” event event at event event event� event event moment event event –\n",
      "Stadisadadasekase?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Lammerammeriammeriammerisammerisendendend\n",
      "\n",
      "Atelier's St. George's St St George'sSt.George's St\n",
      "isisislamisis eating?\n",
      "retretretRetRetRetretRetretretRestRestRestRetRetRestRest\n",
      "000000000urtis kept with with?000000adurtis/000000\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      " the child's party?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "vqa: What is the name of the type of person that would make this food\n",
      "On the plate?\n",
      "Sputputputsputputdown: How many people of this photo?\n",
      "\n",
      "TheAcites in the air?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Stoppoppeladadadendendendadendadelad000000\n",
      "TheRestRestRest Rest Rest RestRest RestRestRestrest RestRestrestRestRest\n",
      "Eladendendendstendendtendenddendendisendend\n",
      "\n",
      "At least from this hot dog?\n",
      "isputputsputtisputs made on?Ikendisput\n",
      "\n",
      "000000000aditalitalital full of?\n",
      "”” event event at event event event� event event moment event event –\n",
      "Lammeritalitalitalier000000000mermermeritalismermer\n",
      "islammer on the floor?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "I/I/Em/impimpac//m/i/m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                        | 0/2 [00:16<?, ?it/s, em=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "”” event event at event event event� event event moment event event –\n",
      "I am in the picture?\n",
      "”” event event at event event event� event event moment event event –\n",
      "\n",
      "\n",
      "I/I/Em/impimpac//m/i/m\n",
      "TheRestRest RestRestRestRestfastRestRestrestRestResteventualRestRest\n",
      "is closer to the camera?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I (not) in this photo?\n",
      "I’m the man standing next to?\n",
      "At a cell phone screen?\n",
      "000000000adendendendammeri000000amperamperi\n",
      "100000000300000000000:000000/0000001000000100\n",
      "\n",
      "I.I.E.000000000.000I.000/000000\n",
      "”” event event at event event event� event event moment event event –\n",
      "is visible?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Theoppeloppoppelappeloppelimpeloppetoppelfast\n",
      "is on the man's shirt?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I’m going to share the bananas?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is painted on the vase?\n",
      "Stoppits match the real ones?\n",
      "”” event event at event event event� event event moment event event –\n",
      "\n",
      "Stoppoppoppendendendisendendtendendstoppendi\n",
      "Eloppoppeloppendeloppelendelendendendelisend\n",
      "is it?\n",
      "I am in the water?\n",
      "asputputputsputputasputeventeventeventfuleventfulfastfast\n",
      "ismerusks are visible?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is this?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "”” event event at event event event� event event moment event event –\n",
      "vqa: Is there enough room under the armchairs for a cat to hide?\n",
      "”” event event at event event event� event event moment event event –\n",
      "Ibital at the clock face?\n",
      "IRestRestRestfastRestRestrestRestRestFastRestResteventfulRestRest\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I was made at the time clock showing?\n",
      "Atoppoppeloppelis sitting on?\n",
      "000000000 at the table?\n",
      "I am in the picture?\n",
      "Stoppel the middle vase?\n",
      "000000000I was made made of?\n",
      " on the table?\n",
      "”” event event at event event event� event event moment event event –\n",
      "TheRestRestRestaurantRestRest RestRestRestrestRestRestfastRestRest\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Istoppelisisislamitalisisendendisendisis\n",
      "islammeri made a deep dish crust?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Stopposed:Stoppendelis /Stoppelis:Stad\n",
      "”” event event at event event event� event event moment event event –\n",
      "\n",
      "reputs are in this image?\n",
      "Istoppoppoppala eating?\n",
      "isputputs under the trees?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I was the elephants full grown?\n",
      "III the the jungle?\n",
      "000000000.000000,000000(000000)000000k000\n",
      "000000000 at the the pizza?IStammeri000000meri\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "000000000adaditalitalitalier name: /000000/000000\n",
      "vqa: What item is folded up and sitting on the edge of the shower stall\n",
      "is on the shower curtain?\n",
      "The following eventful eventful, eventful and eventful events of the countertop\n",
      "tendendelits (tendelis)trends/tend\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "”” event event at event event event� event event moment event event –\n",
      "“I am in the sink?\n",
      "The person on the right called?\n",
      "On the official's chair?\n",
      "The Pretemmeriams are in the photo?\n",
      "“I’m unbroken“\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Ladendendendadend clock say?I was made:Ladad\n",
      "Prostoppelisput 4 colors??\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "At the last letter over the plane?\n",
      "”” event event at event event event� event event moment event event –\n",
      "”” event event at event event event� event event moment event event –\n",
      "\n",
      "Eloppoppeloppeladeloppetoppel.000.000000\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is shown?\n",
      "000000000amis are there?\n",
      "“He’s like this as a child a child”\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "II in the background?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "GETGETGETTING a mosque in this city?\n",
      "II (I)000000000I0000001000000300000000\n",
      "\n",
      "”” event event at event event event� event event moment event event –\n",
      "is this?\n",
      "isisis well seasoned?\n",
      "is shown?\n",
      "\n",
      " kind of food typical?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "is shown?\n",
      "Ibiking on the hot dog?\n",
      "under on the hot dog?\n",
      "\n",
      "The (event) at the highest of this dish?\n",
      "What kind of toppings are on the hot dogs?\n",
      "\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is in the white bowl?\n",
      "is is shown?\n",
      "In the top left of the top of the bottom left left, we are in the\n",
      "I/I/Em/impimpac//m/i/m\n",
      "isle spears are there?\n",
      "I.I.Jappelappeloppeloppurloppelamper\n",
      "tendereputputputsundertundertendeventful(sic)\n",
      "At my drink in this picture?\n",
      "is the hot dog sitting on top of?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "stoppoppeloppel.publicpublicpublicstoppelprosecutedpublic\n",
      "\n",
      "IRestRestRestitalammeriaditalitalitaliammeriammer\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "is the blackened meat on the bread?\n",
      "is this?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "\n",
      "”” event event at event event event� event event moment event event –\n",
      "I (event)I (instammeriaditalitalitalis000000\n",
      "impierlikelike for dessert?000000000meri000000ad000000\n",
      "The (eventful)publicispublicisrespectivelypubliciseventfulpublicis\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                        | 0/2 [00:16<?, ?it/s, em=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "”” moment” event event event moment moment moment event moment event event\n",
      "Ikendendendtendenddendendiendendisendend\n",
      "isputputput on the stomach on the pizza?\n",
      "I was eaten for breakfast?\n",
      "peri from a delicatessen?\n",
      "is on the potatoes?\n",
      "““I’t“t’s’�\n",
      "”” event event at event event event� event event moment event event –\n",
      "Stoppeloppelis the was the fries?\n",
      "”” event event at event event event� event event moment event event –\n",
      "IRestRestRestitalitalitalisendendendelisisislamis\n",
      "000000000amamis are these?\n",
      "the pattern on the vase?\n",
      "Eloppoppoppeloppelkundertendendendred.000000\n",
      "\n",
      "\n",
      "On the toilet tank tank tank\n",
      "GETGETTY: You can’t get the outside?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I have a glass door?\n",
      "I.I.Julululmulmurturtiememem\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "RestRestRest RestRestRestrestRest Rest Rest RestRestrestoppeloppel\n",
      "000000000isis coming from?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I am in this cake?\n",
      " 100000000000.000000,000000ElisElispublicisEl\n",
      "At the time of the event at this event?\n",
      "000000000 at the orange sign?\n",
      "000000000 of the hotel?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I (I) is pictured?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "”” event event at event event event� event event moment event event –\n",
      "is stand resembles what letter?\n",
      "is this?\n",
      "000000000 the picture?\n",
      "The company is represented in the scene?\n",
      "000000000 (000)000000 100000000.000000100000000\n",
      "is this?\n",
      "At the man get down from the machine?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is made made of on wall?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is in the room?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is on the floor?\n",
      "islammeri's motif?\n",
      "000000000 at the the the wall?\n",
      "islamadisendendendadisadisislammeriadis\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I am on the wall?\n",
      "\n",
      "What is the man with the girls wearing?\n",
      "vqa: What color is the man's shirt in the front of the photo?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "000000000/000000 at the street?\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000000adisendendendememid?000000300000000\n",
      "On the left?\n",
      "I'm a street?\n",
      "IRestRestRestitalis/Stoppitalis /Stoppis/Rest\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is it?\n",
      "I.I. I. I, I.000000000,000.000\n",
      "000000000amis are there?\n",
      "\n",
      "I (not) in this photo?\n",
      "I (I)I (Lammer: I (Elis)Elis\n",
      "\n",
      "”” event event at event event event� event event moment event event –\n",
      "tendemputputputsundertunderturlsputsputt\n",
      "I was killed. I was killed on the day I was attacked on the night I\n",
      "I am the person holding?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "In this scene?\n",
      "is there?\n",
      "ispublicpubliclargepublicpublicpublicispublicis.publicpublicaspublicpublicad\n",
      "\n",
      "”” event event at event event event� event event moment event event –\n",
      "I.I.Lammer's (I.000000000/000000\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I’m in the vase next to the flowers?\n",
      "\n",
      "IIIamimpimpimpitalitalitalammeri/000000000\n",
      "Ruson: England's Queen mother and these blooms share what syllable?\n",
      "I am in the picture?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is this?\n",
      "000000300000000000 1000000001000000ad000000am000\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "On the table?\n",
      ".000000000,000000.000, 100,000,100,000\n",
      "isnt appearing as black silhouettes?\n",
      "000000000amis are there?\n",
      "”” event event at event event event� event event moment event event –\n",
      "is in the pan?\n",
      "is the red vegetable in the clear dish?\n",
      "stoppel contain a pie pan?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "000000000 at the left?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Stoppeloppelappelazulmazazulammlazul\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████                                                        | 1/2 [00:16<00:16, 16.74s/it, em=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I/I/Em/impimpac//m/i/m\n",
      "Iammer at the top?\n",
      "I (I)I (Prostoppel )(ProstiProst\n",
      "isis well-groomed?\n",
      "\n",
      "At the top of the building?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "turlurlurlputputputsputput\n",
      "Eloppoppoppeloppelitalitalitalier,000000000,000\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "\n",
      "Eloppoppoppeloppelitalitalitalisendendendisendis\n",
      "At the bottom of the tower?\n",
      "lurti – high high high rise?\n",
      "At the time of this tower made of wood?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Eburtis/ElisElisispublicispublic\n",
      "tendelasputput in this photo?\n",
      "000000000meris standing on?000000300000000E000000ad\n",
      "\n",
      "000000000/000000ad000000am000000.000000m000\n",
      "is' fleece dirty?\n",
      "appelappeloppeloppetoppel(eventful)000000000\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is on the plates?\n",
      "is?\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "Stoppeloppelappel.com.auknotnotElknot\n",
      "?\n",
      "putputputnotputputsputputgetputputtputput\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "Stoppoppeloppelammer's hat?Stoppelappelopp\n",
      "I am in the image?\n",
      "I.I.000000000,000000.000m000000I.\n",
      "I/I/Em/impimpac//m/i/m\n",
      "tendendenddisendendemtendemendendelfastfastfast\n",
      "I was always a leash?\n",
      "““Jeloppoppeloppel.“Eloppel\n",
      "IIam the bird?\n",
      "I am the birds in flight?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "At least I am in the picture?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "\n",
      "On the back of the chair?\n",
      "I.I.Lam's (I)000000000I.000000\n",
      "000000000adurtis is made?\n",
      "Onputput a sponsor of this field?\n",
      "Prostoppoppoppeloppetoppel reputedly made $000000\n",
      "”” event event at event event event� event event moment event event –\n",
      "AtRestRestRestuted/Stopposed/Stammeriam/RestRest\n",
      "\n",
      "Ladendendendiammeriammeriendendammeriammer\n",
      "disrespectspublicoppeloppelitoppelispublicoppetoppel\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is on the top right?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "At once/eventful/disrespectspublicamperiamspublicacred\n",
      "AtRestRestRestfastRestRestrestRestRest RestRestRestFastFastRestRest\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is there?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████                                                        | 1/2 [00:29<00:16, 16.74s/it, em=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is on the plate?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Stoppoppoppelisislammeriaditalismeriammer\n",
      "000000000 at a game?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Iamam a movie?\n",
      "I/RestRestRestendendendis /I /I/I/L\n",
      " the person wearing?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "appendendenddisdisappenditaliamputs eating?\n",
      "I am in the picture?\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "000000000300000000ad000000s down?0000001000000\n",
      "RestRestRest Rest RestRest RestRestRestrestRestRestStopposedRestRest\n",
      "At the end of the day?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Stoppendendendadendenddendendtendendiendend\n",
      "Ladendendend in Dendendi Denditali Stadend\n",
      "\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "isisisendendendisenditalisendisismeriendend\n",
      "000000000i000000adis (sadadis000000is000\n",
      "AtI.I.000000000.000,000000/000000,\n",
      "isputputsputsundertundertiamsburiamtredi\n",
      "Eloppoppoppendendendiams' /Eloppenditaliams\n",
      "\n",
      "putputputsputput(sputs)putsunder(sunder\n",
      "Stopposed:StoppeliElisElisStiStiEl\n",
      "““I’t’“Stopposed“\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "is in the background?\n",
      "\n",
      "reputs are in this photo?\n",
      "Stoppoppeli's tail up?\n",
      "\n",
      "000000000%000000k000000ad000000i000000a000\n",
      "?\n",
      "\n",
      "getting\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000000adisas running?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is the stakes in the dirt?\n",
      "What color shirt is the man standing in the background wearing?\n",
      "000000000adisendendendememid?000000300000000\n",
      "\n",
      "StoppeloppelispublicpublicispublicisPublicisunpublicisun\n",
      "I was made with fondant?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "100000000300000000000c000000stoppeloppoppelis\n",
      "is shown?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Istoppoppeloppelammeri.000000000%000000\n",
      "WHAT DO USED ON THIS CAKE?\n",
      "Stoppoppeloppelausputputputstoppelusputputted\n",
      "I/I/Em/impimpac//m/i/m\n",
      "At the figure on the cupcake?\n",
      "RestRestRest Rest Rest RestRest RestRestRestrest RestRestrestRestRestend\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000 person in the picture?\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "AtRestRestRestitRestRestituRestRestendRestRestElvisRestRest\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000000(000000)000000 100000000 (000000).000\n",
      "under on a soft surface?\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "I(I)Imendendendemendendiammeriendend\n",
      "000000000Stoppopposed.Stopposed:Stoppadis.\n",
      "I (disappitis in there?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "On the coffee table? On the table.On the lunch table. On the coffee\n",
      "”” event event at event event event� event event moment event event –\n",
      "I (I)000000000aditalitalitalammeriad000000\n",
      "I/I/Em/impimpac//m/i/m\n",
      "At least two upside down glasses?\n",
      "”” event event at event event event� event event moment event event –\n",
      "EloppendendendeloppeloppendelendendEloppelend\n",
      "N.I.N.A.R.L.J.Naims\n",
      "I am in the picture?\n",
      "”” event event at event event event� event event moment event event –\n",
      "”” event event at event event event� event event moment event event –\n",
      "000000000 the picture?\n",
      "1.1% of the population of the U.S. with the rest of\n",
      "IStammer the tower?\n",
      "At a clock on the church tower?\n",
      "”” event event at event event event� event event moment event event –\n",
      "IIam a backpack?\n",
      "ThedisdisdisrespectfuldisdisappointingdisdisDisdisrespecting\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "100000000000mer in this scene?\n",
      "is on this house?\n",
      "”” event event at event event event� event event moment event event –\n",
      "000000000amis are there?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000000 made way street street?\n",
      "000000000 at the the bus?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "I(I) (I) I (I )\n",
      "\n",
      "IRestRestRestitalitalitaliams (disappendendendreds\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I (I)I (Lammermeri(sundertammer\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "”” event event at event event event� event event moment event event –\n",
      "I am the man's shadow?\n",
      "”” event event at event event event� event event moment event event –\n",
      "At the court is green?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "100000000300000000000(000)000000.000000100000\n",
      "GeorgiaGeorgia’s “understated” dinner dinner\n",
      "At the bottom of the top of the pole?\n",
      "”” event event at event event event� event event moment event event –\n",
      "Thetendendtendtrendtrendendttrendrendtend\n",
      "At the top?\n",
      "Iammer is in the picture?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████                                                        | 1/2 [00:30<00:16, 16.74s/it, em=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(public)public.publicpublicpublic.compublic.000.000publicpublic\n",
      "\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is the man cooking so many hot dogs?\n",
      "eventevent new or ancient clock?\n",
      "stoppoppel clock read?\n",
      "IJSESESEJSEJIJEJSEpublicispublicis\n",
      "ElisElisI have made personality?\n",
      "The rest of them are in the picture?\n",
      "000000000adurt taken at at?000000/000000urt at?\n",
      "isntoppeloppelammlammeriammeriamtoppelesc\n",
      "is this?\n",
      "On the man doing in the picture?\n",
      "I’m the name of the cartoon man presenting to the cartoon woman?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "At the end of the day with the white arrow say?\n",
      "EladisEladelisElisElaseEladElisisEl\n",
      "What is above the \"No Left Turn\" sign?\n",
      "000000000 at a dress?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Eladurtoppel fall forward?\n",
      "I am a person and a woman?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I get the sun in the picture?\n",
      "II was an infantElElIIIElElEl000000000I\n",
      "Thedisdisdisappelas on the babies shirt?\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "\n",
      "I (not) in this photo?\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "AtRestRestRestoppendendendisendendiamendendemendend\n",
      "tendendenditalitalitaliamsundertenditaliamtendi\n",
      "\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "At the end of the day at the other girl's hair?\n",
      "I/I/Em/impimpac//m/i/m\n",
      " men's shirt?\n",
      ".I.R.L.E.N.St.burburbur.\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "At least they pay for the hot dogs?\n",
      "Elmer's hand?Elmer’s hand?\n",
      "is always on the ground?\n",
      "”” event event at event event event� event event moment event event –\n",
      "000000000meris is white?\n",
      "is on the table?\n",
      " are located?\n",
      "Stoppoppoppeli /Stoppelis /Stadstoppel\n",
      "”” event event at event event event� event event moment event event –\n",
      "Iurturturt in the the background?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000000 at the table?\n",
      "”” event event at event event event� event event moment event event –\n",
      "is it?\n",
      "”” event event at event event event� event event moment event event –\n",
      "On a pillow present in the picture?\n",
      "I’m the best the cat have?\n",
      "RestRestRest Rest Rest RestRest RestRestRestrestRestRestStoppeli\n",
      "I/I/Em/impimpac//m/i/m\n",
      "TheRestRestRestituted/Stopposed/RestRestendendendEl\n",
      "is on the picture?\n",
      "I am in the picture?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "000000000meris standing on?000000300000000E000000ad\n",
      "I'm a boat?\n",
      "I.I.Lam's skin?\n",
      "At the top of the list of the most of the people at the front of the\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "vqa: Is the man in the forefront trying to get signal on a cell phone\n",
      "000000000meris standing on?000000300000000E000000ad\n",
      "At the event this picture was taken?\n",
      "000000000 at the ground?\n",
      "000000000 at the air?\n",
      "What brand of shoes is the skateboarder wearing?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000000adis a performance?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is made in the foreground barefooted?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "At the time of the day of a skateboard?\n",
      "ProProProStoppelProStStadstoppeloppelpublicpublic\n",
      "000000000 the picture?\n",
      "isputputput into jail in this photo?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "is painted on the surfboard?\n",
      "““I’t’s“t”t\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "is in the kite?\n",
      "At the time, the boy's kite?\n",
      "IRestRestRest RestRestRestendRestRestrestRestRestElElRestRest\n",
      "I was made made made in the police made made: I was made:I was\n",
      "000000000adendendendeladendeloppeladisendend\n",
      "”” event event at event event event� event event moment event event –\n",
      "disdisdisputputputsputfastfastfastelitsitsitsalwaysalways\n",
      "\n",
      "I am the women wearing?\n",
      "is wet?\n",
      "as?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000000 at the ground?\n",
      "The Pret Pret Pretis are on the ground?\n",
      "is this?\n",
      "I/I/Em/impimpac//m/i/m\n",
      " digital buttons?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "On a baking tray?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000300000000000 made made with?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "At any number of times on the table?\n",
      "At that this is for a party?\n",
      "000000000 at the the pizza?IStammeri000000meri\n",
      "Liam’s “I’m” at the plate?\n",
      "tendendendtendelammertendemtendisputt\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "”” event event at event event event� event event moment event event –\n",
      "000000000adis light on?\n",
      "vqa: How lucky is it that no one was walking on the sidewalk at that\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Stopposed players wearing black shirts?\n",
      "IRestRestRestauntauntauntsturlsputputputsputsnot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████                                                        | 1/2 [00:30<00:16, 16.74s/it, em=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000adadel batter from?III/I/000000\n",
      "is jugs are visible?\n",
      "Stoppel the picket fence?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "disappear (disdisappevent?\n",
      "000000000.000000,000,100,000.100.000.\n",
      "”” event event at event event event� event event moment event event –\n",
      "At least one person on TV wearing a tie?\n",
      "appear above the TV set?\n",
      "”” event event at event event event� event event moment event event –\n",
      "At this woman's dress?\n",
      "I’tIStammer“I“Lammeri\n",
      "AtRestRestRestInRestRestElisElisRestRestisElElis\n",
      "Stoppelis/Stadelis / Stoppeli/Stopp\n",
      "is on the man's bike?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "Southwest police official's plane?\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000000 at a game?\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "000000000I was made made of?\n",
      "is in a field?\n",
      "is there?\n",
      "\n",
      "I had been to this intersection?\n",
      "\n",
      "RestRestRest RestRestRestoppelis /StoppeliRestResti\n",
      "I’m one of the best-timers for the giraffe to sit\n",
      "Forget the giraffe is in captivity?\n",
      "What is the name of the store behind the lady?\n",
      "At the lady have in her mouth?\n",
      "I.I.Lammer's (I.000000000/000000\n",
      "Stoppoppoppelis/Stoppendis /Stoppadis /\n",
      "000000000a a hybrid?\n",
      "At the popsicle sticks used for?\n",
      "is the man in mid-air?\n",
      "Ibend in the snow?\n",
      "““I’m’t’s’I\n",
      "endend the lighted sign?\n",
      "AtAt at night?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "The best of the best in the world: The best of them, the best of\n",
      "isier on the ground?\n",
      "event of the photo facing downhill?\n",
      "Iamputputs in this photo?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "IRestRestRest RestRest Rest Rest RestRestRestrestRestRested:I\n",
      "I/I/Em/impimpac//m/i/m\n",
      "At the time of the event, I was the most reputed to his surfboard\n",
      "imputed riding a surfboard?\n",
      "is wearing the right type of suit?\n",
      "Propublicaz(publicazPropublicispublicpublicis.compublicisput\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "The Pret Pret Pretend Pret Pretis he wearing?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "AtIStoppitsits shirt?\n",
      "”” event event at event event event� event event moment event event –\n",
      "is kids are wearing black and white?\n",
      "tendendendkendendmendendKendendelkendel\n",
      "\n",
      "RestRestRest RestRest Rest Rest RestRestRestrestRestRestStoppelis\n",
      "“I’t made like like like I’m made made like\n",
      "Elis000000000/000000ElisElis/Elis'000\n",
      "I am very good, I am very bad right?\n",
      "000000000urturturtoutout of?\n",
      "““I’t“.“Stoppelis\n",
      "is the bus in the mirror?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "The (eventful)(public)publicispublicpublicpublicevent(event)\n",
      "000000000/000000adeladelammeriammerisis\n",
      "At the plate?\n",
      "is used in the main dish?\n",
      "I (event)I (instammeriaditalitalitalis000000\n",
      "0000000001000000 (000000)000000(000000ad000\n",
      "000000000 made new new phone?000000300300000000ad000000\n",
      "is that? (bad)isis is that?000000000isisis\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is the text on the sign?\n",
      " the phone case?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I’t““I“t”t’\n",
      "IbitalitalitalIamIIamimpimpimpamamimpam\n",
      "”” event event at event event event� event event moment event event –\n",
      "impimpad go with?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "”” event event at event event event� event event moment event event –\n",
      "IAcadadisis skype?\n",
      "000000000/000000ad000000am000000.000000m000\n",
      "”” event event at event event event� event event moment event event –\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is this?\n",
      "Stoppoppeloppelknotrendelkundertoppelfastfast\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "”” event event at event event event� event event moment event event –\n",
      "At a baseball player holding a bat?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000300000000000 made made made recently?\n",
      "On the other side of the shower curtain?\n",
      "000000000 at the ground?\n",
      "AtRestRestRestInRestRestAtRestInStammeriRestRestEl\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "000000000adisendendendememid?000000300000000\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "IStoppelisisislammeriamisisiamsiamsis\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is in the tile?\n",
      "000000000I was made made of?\n",
      "”” event event at event event event� event event moment event event –\n",
      "”” event event at event event event� event event moment event event –\n",
      "““I’m“.“StretRetret\n",
      "Elurturturt in the bathroom?\n",
      "The ratio of white tiles to blue?\n",
      "is a green plant in this room?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████                                                        | 1/2 [00:30<00:16, 16.74s/it, em=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "”” event event at event event event� event event moment event event –\n",
      "AtRestRestRestendendendisisis/Stoppadisendend\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "\n",
      "What is sitting next to the food?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "badgetgettammertamput food?\n",
      "On the items on the truck?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is on the flatbed?\n",
      "I am the house is this?\n",
      "100000000 made made out of?\n",
      "I (I) (I/I/000000000(I)000000\n",
      "NoRestRestRest RestRestRestrestoppoppeloppendendendElEl\n",
      "”” event event at event event event� event event moment event event –\n",
      "\n",
      "disrespectfuldisrespectrespectfulrespectrespectrespectsrespectrespectablerespectrespectably\n",
      "Stoppoppoppeloppelastoppeladeloppetoppeli\n",
      "I/I/Em/impimpac//m/i/m\n",
      "”” event event at event event event� event event moment event event –\n",
      "I (not) in this photo?\n",
      "At the time of the event at her neck?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "I am in the picture?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Eladisendendendisendiamsendiamendiamisendis\n",
      "”” event event at event event event� event event moment event event –\n",
      "Elis/Stoppel/Stadis/Elis I/I/\n",
      "I/I/Em/impimpac//m/i/m\n",
      "vqa: What three letter word, best describes the liquid container on the ground between\n",
      "On the way on the far left?\n",
      "Stoppel I was made made made I made made it. I made it made\n",
      "\n",
      "”” event event at event event event� event event moment event event –\n",
      "not have a skateboard?\n",
      "““I’m wearing baseball caps”\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "is this?\n",
      "”” event event at event event event� event event moment event event –\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I am in the image?\n",
      "What are the group of people holding in their hand?\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "Atdisdisendendendor on television?\n",
      "I was to fix the train?\n",
      "?I/I/Lamitalis/Stammeri/Stad\n",
      "”” event event at event event event� event event moment event event –\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "\n",
      "Acaditalital bonnet?\n",
      "Stoppeloppel.publicpublicpublicoppelispublicpublicrestoppel\n",
      "\n",
      "”” moment” event event event moment moment moment event moment event event\n",
      "I/I/Em/impimpac//m/i/m\n",
      "On the beach (disappelis on the beach?\n",
      "I (I)000000000 (I/Emeri000000/000\n",
      "\n",
      "(RestRestRest RestRest Rest Rest RestRestRest restRestRestrestRestRest\n",
      "I/I/Em/impimpac//m/i/m\n",
      "Theisputputs in the photo?\n",
      "I/I/Em/impimpac//m/i/m\n",
      "What color is the carpet in the middle of the floor?\n",
      "I always have a low ceiling?\n",
      "I(I)000000000(000)000300000000.000000\n",
      "Stoppelier000000000,000000300000000redi000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:30<00:00, 15.29s/it, em=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eloppoppoppeloppel(public)Eloppelvisit(s\n",
      "\n",
      "I/I/Em/impimpac//m/i/m\n",
      "\n",
      "Iwilliamsiamsundertiamsrespectiamswilliamtiam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from compute_metrics import compute_metrics\n",
    "generations, stats = compute_metrics(\n",
    "    \"vqa\", \"vqa\", reft_model, tokenizer, eval_dataset, eval_dataset,\n",
    "    '', 'test', args.batch_size, \n",
    "    data_collator,\n",
    "    split=False, greedy_decoding=True, temperature=1.0, top_p=None, top_k=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5b870-29e4-4fd5-9f23-429366ed606a",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "\n",
    "1. Speed up data loading [open ended perf problem]\n",
    "2. Checkup the intervention locations for VL-BART\n",
    "3. Fine-tuned model's performance on eval/test VQA\n",
    "4. Fine-tuned model manual validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20d672-63e3-45d1-81c0-9f6540ce6f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

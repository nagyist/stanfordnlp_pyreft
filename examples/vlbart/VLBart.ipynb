{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b4376f-f7a5-4aab-86c8-f25ddef37b82",
   "metadata": {},
   "source": [
    "## VLBart PyReft Integration\n",
    "This is my preliminary try on integrating VLBart with PyReft.\n",
    "### Instructions\n",
    "1. Use Pyvene's peterwz-llava branch and PyReft's peterwz-llava branch.\n",
    "2. Head to pyreft/examples/vlbart/DoRA/image_video_text_understanding, and install packages with the same version as the requirements.txt there. Note that DoRA requires a much less transformers version.\n",
    "3. Download dataset according to the instructions in pyreft/examples/vlbart/DoRA/image_video_text_understanding/README.md, specifically, go to the google drive link and download processed CLIP features. Put it in pyreft/examples/vlbart/DoRA/datasets/ In this notebook we only process on VQA features.\n",
    "4. In image_video_text_understanding/download_backbones.py, change the cache directory to your directory storing the models.\n",
    "5. Try run image_video_text_understanding/VL-T5/scripts/image/dora.sh to see if your DoRA (VLBart model) is installed successfully.\n",
    "6. Run this notebook.\n",
    "### Known Issues\n",
    "1. Directly plugging the DoRA VLBart model here resulted in a 0.20~ VQA performance.\n",
    "2. The training is fast in first few steps, then become very slow. I suspect that is related to the data loading cache behavior. Batching the dataset loading process, instead of the lazy data loading we are using now with ReftDataloaderDataset, may be a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cf6d8-cb3e-49c8-a087-61c9b162a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'DoRA/image_video_text_understanding/VL-T5/src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8704192a-b100-4028-a320-c94079566f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vqa_clip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbad52-fd2b-41e3-b290-7f511452c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_args = {'RefCOCO_BUTD': False,\n",
    " 'RefCOCO_GT': False,\n",
    " 'adam_beta1': 0.9,\n",
    " 'adam_beta2': 0.999,\n",
    " 'adam_eps': 1e-06,\n",
    " 'add_adapter_cross_attn': True,\n",
    " 'add_layer_norm_after_adapter': False,\n",
    " 'add_layer_norm_before_adapter': False,\n",
    " 'additional_visual_embedding_layers': 0,\n",
    " 'answer_normalize': False,\n",
    " 'backbone': 'facebook/bart-base',\n",
    " 'batch_size': 512,\n",
    " 'caption_cocoonly': True,\n",
    " 'caption_only': False,\n",
    " 'classifier': False,\n",
    " 'clip_grad_norm': 5.0,\n",
    " 'cls_task': 'tinyimagenet',\n",
    " 'coco_only': False,\n",
    " 'comment': '',\n",
    " 'decoder_prompt_len': 0,\n",
    " 'deepspeed': None,\n",
    " 'distributed': False,\n",
    " 'do_lower_case': False,\n",
    " 'dora_simple': False,\n",
    " 'downsample': True,\n",
    " 'dropout': 0.1,\n",
    " 'dry': False,\n",
    " 'efficient_unique_hyper_net': False,\n",
    " 'encoder_prompt_len': 0,\n",
    " 'epochs': 20,\n",
    " 'expand_vis_embedding': False,\n",
    " 'factorized_phm': True,\n",
    " 'feat_dim': 2048,\n",
    " 'feature_type': 'RN101',\n",
    " 'fp16': False,\n",
    " 'freeze_bn_statistics': False,\n",
    " 'freeze_ln_statistics': False,\n",
    " 'from_scratch': False,\n",
    " 'full_determinism': False,\n",
    " 'gen_max_length': 20,\n",
    " 'gpu': 0,\n",
    " 'gradient_accumulation_steps': 1,\n",
    " 'ground_upsample': 1,\n",
    " 'ground_weight': 1,\n",
    " 'hypercomplex_division': 4,\n",
    " 'image_size': '(224,224)',\n",
    " 'individual_vis_layer_norm': True,\n",
    " 'itm_cocoonly': True,\n",
    " 'lambda_z': 0.001,\n",
    " 'load': None,\n",
    " 'load_lxmert_qa': None,\n",
    " 'local_rank': 0,\n",
    " 'log_train_accuracy': False,\n",
    " 'lora_alpha': 32,\n",
    " 'lora_dim': 128,\n",
    " 'lora_settings': True,\n",
    " 'losses': 'lm,obj,attr,feat',\n",
    " 'low_rank_rank': 1,\n",
    " 'lr': 0.01,\n",
    " 'max_n_boxes': 36,\n",
    " 'max_text_length': 20,\n",
    " 'mid_dim': 768,\n",
    " 'multiGPU': True,\n",
    " 'multitask_sampling': 'roundrobin',\n",
    " 'n_boxes': 36,\n",
    " 'n_ground': 1,\n",
    " 'n_image_tokens': 4,\n",
    " 'no_prefix': False,\n",
    " 'num_beams': 5,\n",
    " 'num_workers': 4,\n",
    " 'obj_mask_rate': 0.15,\n",
    " 'oneddownsample': False,\n",
    " 'optim': 'adamw',\n",
    " 'optimizer': 'adamw',\n",
    " 'oscar_tags': False,\n",
    " 'output': 'snap/VLBart_multitask/tune+lr1e-2_plzplz2',\n",
    " 'phm_init_range': 0.01,\n",
    " 'phm_rank': 1,\n",
    " 'pos_dim': 4,\n",
    " 'post_prompt': '',\n",
    " 'prefix': None,\n",
    " 'project_name': 'RN101_LMsingle_dora_128_bs300_image224_lora_settings',\n",
    " 'projected_task_embedding_dim': -1,\n",
    " 'prompt': 'vqa: ',\n",
    " 'raw_label': False,\n",
    " 'reduction_factor': 16,\n",
    " 'remove_bn_vis_adapter': False,\n",
    " 'run_name': 'tune+lr1e-2_plzplz2',\n",
    " 'seed': 9595,\n",
    " 'share_down_sampler': False,\n",
    " 'share_up_sampler': False,\n",
    " 'share_vis_lang_layer_norm': False,\n",
    " 'shared_phm_rule': True,\n",
    " 'shared_phm_rule_over_tasks': False,\n",
    " 'shuffle_boxes': False,\n",
    " 'single_vqa_prefix': False,\n",
    " 'sparse_sample': False,\n",
    " 'submit': False,\n",
    " 'tasks': 'vqa',\n",
    " 'test': None,\n",
    " 'test_answerable': False,\n",
    " 'test_only': False,\n",
    " 'testing': False,\n",
    " 'tokenizer': None,\n",
    " 'track_z': False,\n",
    " 'train': 'train',\n",
    " 'train_topk': -1,\n",
    " 'unfreeze_batch_norms': False,\n",
    " 'unfreeze_bias': False,\n",
    " 'unfreeze_decoder_layer_norms': False,\n",
    " 'unfreeze_encoder_layer_norms': False,\n",
    " 'unfreeze_language_model': False,\n",
    " 'unfreeze_layer_norms': False,\n",
    " 'unfreeze_lm_head': False,\n",
    " 'unfreeze_vis_encoder': False,\n",
    " 'unfreeze_vis_last_layer': False,\n",
    " 'unique_hyper_net': False,\n",
    " 'use_adam_for_visual': False,\n",
    " 'use_adapter': False,\n",
    " 'use_attn_prefix': False,\n",
    " 'use_compacter': False,\n",
    " 'use_data_augmentation': False,\n",
    " 'use_dora': False,\n",
    " 'use_hyperformer': False,\n",
    " 'use_lm_head_adapter': False,\n",
    " 'use_lora': False,\n",
    " 'use_lradapter': False,\n",
    " 'use_separate_optimizer_for_visual': False,\n",
    " 'use_single_adapter': False,\n",
    " 'use_single_lora': False,\n",
    " 'use_single_prompt': False,\n",
    " 'use_tasks_prompts': True,\n",
    " 'use_vis_adapter': False,\n",
    " 'use_vis_layer_norm': True,\n",
    " 'use_vis_order_embedding': True,\n",
    " 'use_vision': True,\n",
    " 'valid': 'valid',\n",
    " 'valid_batch_size': 512,\n",
    " 'valid_topk': -1,\n",
    " 'vis_adapter_type': 'middle-bottleneck',\n",
    " 'vis_lr': 0.0001,\n",
    " 'vis_pointer': False,\n",
    " 'vis_pooling_output': False,\n",
    " 'vis_reduction_factor': 2,\n",
    " 'vis_use_transformer': False,\n",
    " 'vis_weight_decay': 0.01,\n",
    " 'warmup_ratio': 0.1,\n",
    " 'weight_decay': 0.01,\n",
    " 'word_mask_rate': 0.15,\n",
    " 'world_size': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d4858-ed49-435b-af99-558330dc0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace(**vqa_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ea1f7-b855-4159-9b84-a3819844b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = []\n",
    "vqa_train_loader = vqa_clip_data.get_loader(\n",
    "    args,\n",
    "    split='karpathy_train', mode='train', batch_size=args.batch_size,\n",
    "    distributed=args.distributed, gpu=0,\n",
    "    workers=args.num_workers,\n",
    "    topk=args.train_topk,\n",
    ")\n",
    "train_loaders.append(vqa_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dfab9b-e1a0-4765-9676-dc9001117163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyreft.dataset import ReftDataset, ReftDataloaderDataset\n",
    "from pyreft import (\n",
    "    ReftTrainerForCausalLM, \n",
    "    ReftDataCollator,\n",
    "    LoreftIntervention,\n",
    "    TaskType,\n",
    "    ReftConfig,\n",
    "    get_reft_model,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b07f38-308c-40ed-aab2-1c98eb9e041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLBartDataset(ReftDataloaderDataset):\n",
    "    \"\"\"\n",
    "    A ReftClassificationDataset only contains a single text field\n",
    "    that we tokenize, intervene on a prefix + suffix of, and\n",
    "    compute subspace settings for. This is intended for classification\n",
    "    tasks.\n",
    "\n",
    "    Remember to pass in the input_field and label_field as kwargs.\n",
    "    \"\"\"\n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load the dataset (or a portion of it) from HF or a local file.\"\"\"\n",
    "\n",
    "        self.task_dataset = self.dataloader.dataset\n",
    "        self.collate_fn = self.task_dataset.collate_fn\n",
    "        self.fields_to_pad = [\"input_ids\", \"target_ids\"]\n",
    "        self.pad_mode = \"first\"\n",
    "\n",
    "        # select n random examples if specificed\n",
    "        if self.max_n_example is not None:\n",
    "            self.task_dataset = torch.utils.data.Subset(self.task_dataset, list(range(self.max_n_example)))\n",
    "\n",
    "        # save raw_dataset pointer for access raw strings\n",
    "        self.raw_dataset = self.task_dataset if self.data_split != \"train\" else None\n",
    "        return self.task_dataset\n",
    "\n",
    "    def preprocess(self, kwargs):\n",
    "        self.input_field = \"input_ids\"\n",
    "        self.label_field = \"target_ids\"\n",
    "\n",
    "    def tokenize(self, data_item):\n",
    "        result = {**data_item}\n",
    "        result[\"input_length\"] += 1\n",
    "        result[\"target_length\"] += 1\n",
    "        result[\"instruction\"] = tokenizer.decode(result[\"input_ids\"], skip_special_tokens=True)\n",
    "\n",
    "        # TODO: whether to add \"-1\"?\n",
    "        last_position = len(data_item[self.input_field]) - 1\n",
    "        return result, last_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da558d5-3d05-4178-95b3-bf6c5be3207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, TrainingArguments\n",
    "tokenizer = BartTokenizer.from_pretrained(\n",
    "    args.backbone,\n",
    "    max_length=args.max_text_length,\n",
    "    do_lower_case=args.do_lower_case\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9996cb2-b319-4e3a-a43a-b13003c4efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0,1,2,3,4,5]\n",
    "position = \"f7+l7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168b173-b7ec-492b-aaf5-dfb115011505",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VLBartDataset(\n",
    "    \"vqa\", \n",
    "    tokenizer, data_split=\"train\", \n",
    "    dataloader=vqa_train_loader,\n",
    "    max_n_example=1000,\n",
    "    **{\"num_interventions\": len(layers), \"position\": position, \n",
    "       \"share_weights\": True, \"test_split\": \"validation\"}\n",
    ")\n",
    "eval_dataset = VLBartDataset(\n",
    "    \"vqa\", \n",
    "    tokenizer, data_split=\"val\", \n",
    "    dataloader=vqa_train_loader,\n",
    "    max_n_example=100,\n",
    "    **{\"num_interventions\": len(layers), \"position\": position, \n",
    "       \"share_weights\": True, \"test_split\": \"validation\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b904dc-61c7-45bb-a3d8-f60bdfaf3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitask import Trainer\n",
    "trainer = Trainer(args, vqa_train_loader, None, None, train=True)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52018fc-388d-45e3-a842-c7b953a6eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11518fa-da9f-47d1-993f-e0fa27810f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64daf75f-7f3e-432d-9e33-75ea90d4cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2903c7a-0a60-40f5-9d9c-6509dc891d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vqa_train_loader.dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae73144-b449-41a4-b7d1-1033117717f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForSeq2Seq\n",
    "# data_collator_fn = DataCollatorForSeq2Seq(\n",
    "#     tokenizer=tokenizer,\n",
    "#     model=model,\n",
    "#     label_pad_token_id=-100,\n",
    "#     padding=\"longest\"\n",
    "# )\n",
    "import transformers\n",
    "def keep_intervention_locations(datum):\n",
    "    new_data = {}\n",
    "    new_data[\"input_ids\"] = datum[\"input_ids\"]\n",
    "    # new_data[\"instruction\"] = datum[\"instruction\"]\n",
    "    new_data[\"intervention_locations\"] = datum[\"intervention_locations\"]\n",
    "    new_data[\"attention_mask\"] = datum[\"attention_mask\"]\n",
    "    return new_data\n",
    "\n",
    "def custom_collate_fn(data):\n",
    "    collate_fn_1 = train_dataset.collate_fn\n",
    "    collate_fn_2 = transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    # for item in data:\n",
    "    #     print(item[\"input_ids\"].shape)\n",
    "    output_1 = collate_fn_1(data)\n",
    "    custom_data = [keep_intervention_locations(item) for item in data]\n",
    "    output_2 = collate_fn_2(custom_data)\n",
    "    output = output_1\n",
    "    output[\"intervention_locations\"] = output_2[\"intervention_locations\"]\n",
    "    # print(output[\"intervention_locations\"].shape)\n",
    "    print(torch.max(output[\"intervention_locations\"]))\n",
    "    # Offset image tokens' concatenation\n",
    "    output[\"intervention_locations\"][:,:,-1] += args.n_boxes\n",
    "    print(torch.max(output[\"intervention_locations\"]))\n",
    "    # print(output[\"intervention_locations\"])\n",
    "\n",
    "    # output[\"id\"] = output_2[\"id\"]\n",
    "    # output[\"labels\"] = output_2[\"labels\"]\n",
    "    \n",
    "    output[\"attention_mask\"] = output_2[\"attention_mask\"]\n",
    "\n",
    "    ids = []\n",
    "    instructions = []\n",
    "    for d in data:\n",
    "        ids.append(d[\"id\"])\n",
    "        instructions.append(d[\"instruction\"])\n",
    "    import numpy as np\n",
    "    output[\"id\"] = np.array(ids)\n",
    "    output[\"instruction\"] = instructions\n",
    "    \n",
    "    output[\"logits\"] = output[\"labels\"]\n",
    "    output[\"labels\"] = output[\"target_ids\"]\n",
    "    # output[\"instruction\"] = tokenizer.batch_decode(output[\"input_ids\"], skip_special_tokens=True)\n",
    "    # print(\"Output Keys:\", output.keys())\n",
    "    \n",
    "    # print(\"Input IDs:\", output[\"input_ids\"], tokenizer.batch_decode(output[\"input_ids\"], skip_special_tokens=True))\n",
    "    # print(\"Labels:\", output[\"labels\"].shape)\n",
    "    # labels = [[token for token in sequence if token != -100] for sequence in output[\"labels\"].tolist()]\n",
    "    # print(\"Labels:\", tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "    # print(\"Question IDs:\", output[\"question_ids\"])\n",
    "    # print(\"Answers:\", output[\"answers\"])\n",
    "    # print(\"All answers:\", output[\"all_answers\"])\n",
    "    # print(\"Scores:\", output[\"scores\"])\n",
    "\n",
    "    return output\n",
    "\n",
    "data_collator = ReftDataCollator(data_collator=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497868a-120f-4abe-8bc7-bc1b9cbb264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1\n",
    "dropout=0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e60d3-61d0-48ca-bf94-b787879b1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [{\n",
    "    \"layer\": l, \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": rank,\n",
    "    \"intervention\": LoreftIntervention(\n",
    "        embed_dim=model.config.d_model, low_rank_dimension=rank,\n",
    "        dropout=dropout, dtype=torch.float32, act_fn=None, device=\"cuda\",\n",
    "        add_bias=True\n",
    "    )\n",
    "} for l in layers]\n",
    "task_type=TaskType.CAUSAL_LM\n",
    "\n",
    "reft_config = ReftConfig(representations=representations)\n",
    "empty_reft_config = ReftConfig(representations=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271831c-7645-4ca5-845f-7b5a4caf8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reft_model = get_reft_model(model, reft_config)\n",
    "empty_reft_model = get_reft_model(model, empty_reft_config)\n",
    "empty_reft_model.print_trainable_parameters()\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673c723-0a55-4b39-b261-b0bced33e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"random\",\n",
    "    run_name=\"random\",\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    "    per_device_eval_batch_size=args.batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"no\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    metric_for_best_model=None,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_total_limit=1, # for GLUE, it will save 2 at max.\n",
    "    logging_steps=1,\n",
    "    learning_rate=4e-2,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    "    use_cpu=False,\n",
    "    seed=42,\n",
    "    # until HF supports ReFT, this remains False! :)\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce2fca-1c68-40bd-b143-a0d5267bb3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvene import IntervenableModel\n",
    "# from overrides import overrides\n",
    "\n",
    "class MyTrainer(ReftTrainerForCausalLM):\n",
    "    # @overrides\n",
    "    def training_step(self, model, batch):\n",
    "        # print(\"My trainer step\")\n",
    "        batch = self._prepare_inputs(batch)\n",
    "\n",
    "        # print(\"Batch:\", batch.keys())\n",
    "        device = batch['input_ids'].device\n",
    "\n",
    "        batch = model.model.vis_forward(batch, device)\n",
    "        task = batch[\"task\"]\n",
    "\n",
    "        vis_feats = batch['vis_feats']\n",
    "        input_ids = batch['input_ids']\n",
    "        vis_pos = batch['boxes']\n",
    "\n",
    "        lm_labels = batch[\"target_ids\"].to(device)\n",
    "\n",
    "        inputs = {**batch}\n",
    "        inputs[\"return_dict\"] = True\n",
    "        inputs[\"reduce_loss\"] = False\n",
    "        inputs[\"vis_inputs\"] = (vis_feats, vis_pos)\n",
    "        # print(inputs.keys())\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            self.accelerator.backward(loss)\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        intervenable: IntervenableModel,\n",
    "        inputs,\n",
    "        return_outputs=False\n",
    "    ):\n",
    "        \n",
    "        lm_labels = inputs[\"target_ids\"]\n",
    "        # print(\"KEYS:\", inputs.keys())\n",
    "        # print(\"LABELS:\", lm_labels)\n",
    "        # print(\"SCORES:\", inputs[\"scores\"])\n",
    "        _, cf_outputs = intervenable(\n",
    "            {\n",
    "                \"input_ids\": inputs[\"input_ids\"],\n",
    "                \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                \"vis_inputs\": inputs[\"vis_inputs\"],\n",
    "                \"task\": \"vqa\",\n",
    "                \n",
    "            },\n",
    "            unit_locations={\"sources->base\": (\n",
    "                None,\n",
    "                inputs[\"intervention_locations\"].permute(1, 0, 2).tolist()\n",
    "            )},\n",
    "            labels=inputs[\"labels\"],\n",
    "            subspaces=inputs[\"subspaces\"].permute(1, 0, 2).tolist() if \"subspaces\" in inputs else None\n",
    "        )\n",
    "        # return\n",
    "        loss = (cf_outputs.loss, cf_outputs) if return_outputs else cf_outputs.loss\n",
    "        if isinstance(loss, tuple):\n",
    "            loss = loss[0]\n",
    "        lm_mask = (lm_labels != -100).float()\n",
    "        B, L = lm_labels.size()\n",
    "\n",
    "        loss = loss.view(B, L) * lm_mask\n",
    "\n",
    "        loss = loss.sum(dim=1) / lm_mask.sum(dim=1).clamp(min=1)  # B\n",
    "\n",
    "        loss = loss * inputs[\"scores\"]\n",
    "\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5201485-07d7-4dfc-b8ac-b64c1edeffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(\n",
    "    model=reft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d479808-d1a7-4343-8307-48a65eb0ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reft_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e94b8-3d45-430d-91bd-10ef4484967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f7722-e83d-4543-811e-faa6345179b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(\"tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943603b8-ecdc-43aa-8c69-546bed34016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyreft\n",
    "# reft_model = pyreft.ReftModel.load(\n",
    "#     \"temp-outputs\", model\n",
    "# )\n",
    "# reft_model.set_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf162a-1788-48dc-9ac9-bd2bb76cb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "reft_model.model.eval()\n",
    "for k,v in reft_model.interventions.items():\n",
    "    _ = v[0].eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e19b8-bb8b-456e-b05c-b3e0ad97bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compute_metrics import compute_metrics\n",
    "generations, stats = compute_metrics(\n",
    "    \"vqa\", \"vqa\", reft_model, tokenizer, train_dataset, train_dataset,\n",
    "    '', 'test', 64, # batch_size\n",
    "    data_collator,\n",
    "    split=False, greedy_decoding=True, temperature=1.0, top_p=None, top_k=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b2551-67c7-4603-a91e-43f9e555d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset[3][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbfba6-7c53-431e-a2de-c27a0d23cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generations[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80133794-e079-4c06-8de3-c8a6a076db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a539b17-812f-4144-90a5-d5d9e6be99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reft_model.save('temp-outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5b870-29e4-4fd5-9f23-429366ed606a",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "\n",
    "1. Speed up data loading [open ended perf problem]\n",
    "2. Checkup the intervention locations for VL-BART\n",
    "3. Fine-tuned model's performance on eval/test VQA\n",
    "4. Fine-tuned model manual validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20d672-63e3-45d1-81c0-9f6540ce6f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

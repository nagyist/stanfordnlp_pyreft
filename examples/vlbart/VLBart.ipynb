{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b4376f-f7a5-4aab-86c8-f25ddef37b82",
   "metadata": {},
   "source": [
    "## VLBart PyReft Integration\n",
    "This is my preliminary try on integrating VLBart with PyReft.\n",
    "### Instructions\n",
    "1. Use Pyvene's peterwz-llava branch and PyReft's peterwz-llava branch.\n",
    "2. Head to pyreft/examples/vlbart/DoRA/image_video_text_understanding, and install packages with the same version as the requirements.txt there. Note that DoRA requires a much less transformers version.\n",
    "3. Download dataset according to the instructions in pyreft/examples/vlbart/DoRA/image_video_text_understanding/README.md, specifically, go to the google drive link and download processed CLIP features. Put it in pyreft/examples/vlbart/DoRA/datasets/ In this notebook we only process on VQA features.\n",
    "4. In image_video_text_understanding/download_backbones.py, change the cache directory to your directory storing the models.\n",
    "5. Try run image_video_text_understanding/VL-T5/scripts/image/dora.sh to see if your DoRA (VLBart model) is installed successfully.\n",
    "6. Run this notebook.\n",
    "### Known Issues\n",
    "1. Directly plugging the DoRA VLBart model here resulted in a 0.20~ VQA performance.\n",
    "2. The training is fast in first few steps, then become very slow. I suspect that is related to the data loading cache behavior. Batching the dataset loading process, instead of the lazy data loading we are using now with ReftDataloaderDataset, may be a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646cf6d8-cb3e-49c8-a087-61c9b162a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'DoRA/image_video_text_understanding/VL-T5/src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8704192a-b100-4028-a320-c94079566f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vqa_clip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dbad52-fd2b-41e3-b290-7f511452c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "vqa_args = {'RefCOCO_BUTD': False,\n",
    " 'RefCOCO_GT': False,\n",
    " 'adam_beta1': 0.9,\n",
    " 'adam_beta2': 0.999,\n",
    " 'adam_eps': 1e-06,\n",
    " 'add_adapter_cross_attn': True,\n",
    " 'add_layer_norm_after_adapter': False,\n",
    " 'add_layer_norm_before_adapter': False,\n",
    " 'additional_visual_embedding_layers': 0,\n",
    " 'answer_normalize': False,\n",
    " 'backbone': 'facebook/bart-base',\n",
    " 'batch_size': batch_size,\n",
    " 'caption_cocoonly': True,\n",
    " 'caption_only': False,\n",
    " 'classifier': False,\n",
    " 'clip_grad_norm': 5.0,\n",
    " 'cls_task': 'tinyimagenet',\n",
    " 'coco_only': False,\n",
    " 'comment': '',\n",
    " 'decoder_prompt_len': 0,\n",
    " 'deepspeed': None,\n",
    " 'distributed': False,\n",
    " 'do_lower_case': False,\n",
    " 'dora_simple': False,\n",
    " 'downsample': True,\n",
    " 'dropout': 0.00,\n",
    " 'dry': False,\n",
    " 'efficient_unique_hyper_net': False,\n",
    " 'encoder_prompt_len': 0,\n",
    " 'epochs': 20, # 100\n",
    " 'expand_vis_embedding': False,\n",
    " 'factorized_phm': True,\n",
    " 'feat_dim': 2048,\n",
    " 'feature_type': 'RN101', # RN101\n",
    " 'fp16': False,\n",
    " 'freeze_bn_statistics': False,\n",
    " 'freeze_ln_statistics': False,\n",
    " 'from_scratch': False,\n",
    " 'full_determinism': False,\n",
    " 'gen_max_length': 20,\n",
    " 'gpu': 0,\n",
    " 'gradient_accumulation_steps': 1,\n",
    " 'ground_upsample': 1,\n",
    " 'ground_weight': 1,\n",
    " 'hypercomplex_division': 4,\n",
    " 'image_size': '(224,224)',\n",
    " 'individual_vis_layer_norm': True,\n",
    " 'is_wandb': False, # True\n",
    " 'itm_cocoonly': True,\n",
    " 'lambda_z': 0.001,\n",
    " 'load': None,\n",
    " 'load_lxmert_qa': None,\n",
    " 'local_rank': 0,\n",
    " 'log_train_accuracy': False,\n",
    " 'lora_alpha': 32,\n",
    " 'lora_dim': 128,\n",
    " 'lora_settings': True,\n",
    " 'losses': 'lm,obj,attr,feat',\n",
    " 'low_rank_rank': 1,\n",
    " 'lr': 1e-2,\n",
    " 'lr_scheduler_type': \"linear\",\n",
    " 'max_n_boxes': 36,\n",
    " 'max_n_train_examples': 50000, # 1000\n",
    " 'max_n_eval_examples': 2000, # 1000\n",
    " 'max_text_length': 20,\n",
    " 'mid_dim': 768,\n",
    " 'multiGPU': True,\n",
    " 'multitask_sampling': 'roundrobin',\n",
    " 'n_boxes': 36,\n",
    " 'n_ground': 1,\n",
    " 'n_image_tokens': 4,\n",
    " 'no_prefix': False,\n",
    " 'num_beams': 5,\n",
    " 'num_workers': 4,\n",
    " 'obj_mask_rate': 0.15,\n",
    " 'oneddownsample': False,\n",
    " 'optim': 'adamw',\n",
    " 'optimizer': 'adamw',\n",
    " 'oscar_tags': False,\n",
    " 'output': 'snap/VLBart_multitask/tune+lr1e-2_1e-2',\n",
    " 'phm_init_range': 0.01,\n",
    " 'phm_rank': 1,\n",
    " 'pos_dim': 4,\n",
    " 'position': 'f11+l11',\n",
    " 'post_prompt': '',\n",
    " 'prefix': None,\n",
    " 'project_name': 'RN101_LMsingle_dora_128_bs300_image224_lora_settings',\n",
    " 'projected_task_embedding_dim': -1,\n",
    " 'prompt': 'vqa: ',\n",
    " 'rank': 1,\n",
    " 'raw_label': False,\n",
    " 'reduction_factor': 16,\n",
    " 'remove_bn_vis_adapter': False,\n",
    " 'run_name': 'tune+lr1e-2_plzplz2',\n",
    " 'seed': 9595,\n",
    " 'share_down_sampler': False,\n",
    " 'share_up_sampler': False,\n",
    " 'share_vis_lang_layer_norm': False,\n",
    " 'share_weights': True,\n",
    " 'shared_phm_rule': True,\n",
    " 'shared_phm_rule_over_tasks': False,\n",
    " 'shuffle_boxes': False,\n",
    " 'single_vqa_prefix': False,\n",
    " 'sparse_sample': False,\n",
    " 'submit': False,\n",
    " 'tasks': 'vqa',\n",
    " 'test': None,\n",
    " 'test_answerable': False,\n",
    " 'test_only': False,\n",
    " 'testing': False,\n",
    " 'tokenizer': None,\n",
    " 'track_z': False,\n",
    " 'train': 'train',\n",
    " 'train_topk': -1,\n",
    " 'unfreeze_batch_norms': False,\n",
    " 'unfreeze_bias': False,\n",
    " 'unfreeze_decoder_layer_norms': False,\n",
    " 'unfreeze_encoder_layer_norms': False,\n",
    " 'unfreeze_language_model': False,\n",
    " 'unfreeze_layer_norms': False,\n",
    " 'unfreeze_lm_head': False,\n",
    " 'unfreeze_vis_encoder': False,\n",
    " 'unfreeze_vis_last_layer': False,\n",
    " 'unique_hyper_net': False,\n",
    " 'use_adam_for_visual': False,\n",
    " 'use_adapter': False,\n",
    " 'use_attn_prefix': False,\n",
    " 'use_compacter': False,\n",
    " 'use_data_augmentation': False,\n",
    " 'use_dora': False,\n",
    " 'use_hyperformer': False,\n",
    " 'use_lm_head_adapter': False,\n",
    " 'use_lora': False,\n",
    " 'use_lradapter': False,\n",
    " 'use_separate_optimizer_for_visual': False,\n",
    " 'use_single_adapter': False,\n",
    " 'use_single_lora': False,\n",
    " 'use_single_prompt': False,\n",
    " 'use_tasks_prompts': True,\n",
    " 'use_vis_adapter': False,\n",
    " 'use_vis_layer_norm': True,\n",
    " 'use_vis_order_embedding': True,\n",
    " 'use_vision': True,\n",
    " 'valid': 'valid',\n",
    " 'valid_batch_size': batch_size,\n",
    " 'valid_topk': -1,\n",
    " 'vis_adapter_type': 'middle-bottleneck',\n",
    " 'vis_lr': 0.00001,\n",
    " 'vis_pointer': False,\n",
    " 'vis_pooling_output': False,\n",
    " 'vis_reduction_factor': 2,\n",
    " 'vis_use_transformer': False,\n",
    " 'vis_weight_decay': 0.01,\n",
    " 'warmup_ratio': 0.00,\n",
    " 'wandb_proj': \"Reft\",\n",
    " 'wandb_name': \"peterwz\",\n",
    " 'wandb_dir': \"wandb\",\n",
    " 'weight_decay': 0.005,\n",
    " 'word_mask_rate': 0.15,\n",
    " 'world_size': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "849d4858-ed49-435b-af99-558330dc0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace(**vqa_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956ea1f7-b855-4159-9b84-a3819844b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 605102 data from split(s) karpathy_train.\n",
      "# Answers: 3129\n",
      "Data sources:  ['karpathy_train']\n",
      "Loaded 605102 data from karpathy_train\n",
      "# all sentences: 605102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 26729 data from split(s) karpathy_val.\n",
      "# Answers: 3129\n",
      "Data sources:  ['karpathy_val']\n",
      "Loaded 26729 data from karpathy_val\n",
      "# all sentences: 26729\n"
     ]
    }
   ],
   "source": [
    "train_loaders = []\n",
    "vqa_train_loader = vqa_clip_data.get_loader(\n",
    "    args,\n",
    "    split='karpathy_train', mode='train', batch_size=args.batch_size,\n",
    "    distributed=args.distributed, gpu=0,\n",
    "    workers=args.num_workers,\n",
    "    topk=args.train_topk,\n",
    ")\n",
    "vqa_val_loader = vqa_clip_data.get_loader(\n",
    "    args,\n",
    "    split='karpathy_val', mode='val', batch_size=args.batch_size,\n",
    "    distributed=args.distributed, gpu=0,\n",
    "    workers=args.num_workers,\n",
    "    topk=args.train_topk,\n",
    ")\n",
    "train_loaders.append(vqa_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b904dc-61c7-45bb-a3d8-f60bdfaf3e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model at GPU 0\n",
      "Model Launching at GPU 0\n",
      "model.encoder.visual_embedding.feat_embedding.0.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.0.bias is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.bias is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.weight is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.bias is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.weight is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.bias is trainable...\n",
      "model.encoder.visual_embedding.img_order_embedding.weight is trainable...\n",
      "VLBartMultiTask(\n",
      "  (model): VLBartModel(\n",
      "    (shared): Embedding(50465, 768)\n",
      "    (encoder): JointEncoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (visual_embedding): VisualEmbedding(\n",
      "        (feat_embedding): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (absolute_vis_pos_embedding): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (obj_order_embedding): Embedding(50465, 768)\n",
      "        (img_order_embedding): Embedding(2, 768)\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "      )\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50465, bias=False)\n",
      ")\n",
      "VLBartMultiTask(\n",
      "  (model): VLBartModel(\n",
      "    (shared): Embedding(50465, 768)\n",
      "    (encoder): JointEncoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (visual_embedding): VisualEmbedding(\n",
      "        (feat_embedding): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (absolute_vis_pos_embedding): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (obj_order_embedding): Embedding(50465, 768)\n",
      "        (img_order_embedding): Embedding(2, 768)\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "      )\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50465, bias=False)\n",
      ")\n",
      "Trainable param percentage: 1.12% (1582848/141156864)\n",
      "Building Optimizer\n",
      "Batch per epoch: 4728\n",
      "Total Iters: 94560\n",
      "Warmup ratio: 0.0\n",
      "Warm up Iters: 0\n",
      "It took 0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from multitask import Trainer\n",
    "trainer = Trainer(args, vqa_train_loader, None, None, train=True)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4dfab9b-e1a0-4765-9676-dc9001117163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyreft.dataset import ReftDataset, ReftDataloaderDataset\n",
    "from pyreft import (\n",
    "    ReftTrainerForCausalLM, \n",
    "    ReftDataCollator,\n",
    "    LoreftIntervention,\n",
    "    TaskType,\n",
    "    ReftConfig,\n",
    "    get_reft_model,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da558d5-3d05-4178-95b3-bf6c5be3207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, TrainingArguments\n",
    "tokenizer = trainer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b07f38-308c-40ed-aab2-1c98eb9e041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VLBartDataset(ReftDataloaderDataset):\n",
    "class VLBartDataset(ReftDataset):\n",
    "    \"\"\"\n",
    "    A ReftClassificationDataset only contains a single text field\n",
    "    that we tokenize, intervene on a prefix + suffix of, and\n",
    "    compute subspace settings for. This is intended for classification\n",
    "    tasks.\n",
    "\n",
    "    Remember to pass in the input_field and label_field as kwargs.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, task, \n",
    "        tokenizer,\n",
    "        data_split=\"train\", dataloader=None, \n",
    "        max_n_example=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.dataloader = dataloader\n",
    "        super(VLBartDataset, self).__init__(task, \"\", tokenizer, data_split, None, 42, max_n_example,\n",
    "        **kwargs)\n",
    "\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load the dataset (or a portion of it) from HF or a local file.\"\"\"\n",
    "\n",
    "        self.task_dataset = self.dataloader.dataset\n",
    "        self.collate_fn = self.task_dataset.collate_fn\n",
    "        self.fields_to_pad = [\"input_ids\", \"target_ids\"]\n",
    "        self.pad_mode = \"none\"\n",
    "\n",
    "        # select n random examples if specificed\n",
    "        if self.max_n_example is not None:\n",
    "            self.task_dataset = torch.utils.data.Subset(self.task_dataset, list(range(self.max_n_example)))\n",
    "\n",
    "        # save raw_dataset pointer for access raw strings\n",
    "        self.raw_dataset = self.task_dataset if self.data_split != \"train\" else None\n",
    "        return self.task_dataset\n",
    "\n",
    "    def preprocess(self, kwargs):\n",
    "        self.input_field = \"input_ids\"\n",
    "        self.label_field = \"target_ids\"\n",
    "\n",
    "    def tokenize(self, data_item):\n",
    "        result = {**data_item}\n",
    "        # result[\"input_length\"] += 1\n",
    "        # result[\"target_length\"] += 1\n",
    "        result[\"instruction\"] = self.task + \": \" + result[\"sent\"]\n",
    "        # print(\"Instruction\", result[\"instruction\"])\n",
    "        # print(\"Sent\", result[\"sent\"])\n",
    "\n",
    "        # TODO: whether to add \"-1\"?\n",
    "        last_position = len(data_item[self.input_field]) - 1\n",
    "        return result, last_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9996cb2-b319-4e3a-a43a-b13003c4efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0,1,2,3,4,5]\n",
    "position = args.position\n",
    "from pyreft.dataset import parse_positions\n",
    "first_pos, last_pos = parse_positions(position)\n",
    "if \"+\" in position and not args.share_weights:\n",
    "    layers += layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2168b173-b7ec-492b-aaf5-dfb115011505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:53<00:00, 440.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:03<00:00, 505.23it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VLBartDataset(\n",
    "    \"vqa\", \n",
    "    tokenizer, data_split=\"train\", \n",
    "    dataloader=vqa_train_loader,\n",
    "    max_n_example=args.max_n_train_examples,\n",
    "    **{\"num_interventions\": len(layers), \"position\": position, \n",
    "       \"share_weights\": args.share_weights, \"test_split\": \"validation\",\n",
    "      \"last_offset\": args.n_boxes}\n",
    ")\n",
    "eval_dataset = VLBartDataset(\n",
    "    \"vqa\", \n",
    "    tokenizer, data_split=\"val\", \n",
    "    dataloader=vqa_val_loader,\n",
    "    max_n_example=args.max_n_eval_examples,\n",
    "    **{\"num_interventions\": len(layers), \"position\": position, \n",
    "       \"share_weights\": args.share_weights, \"test_split\": \"validation\",\n",
    "      \"last_offset\": args.n_boxes}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d52018fc-388d-45e3-a842-c7b953a6eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c11518fa-da9f-47d1-993f-e0fa27810f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartConfig {\n",
      "  \"RefCOCO_BUTD\": false,\n",
      "  \"RefCOCO_GT\": false,\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"adam_beta1\": 0.9,\n",
      "  \"adam_beta2\": 0.999,\n",
      "  \"adam_eps\": 1e-06,\n",
      "  \"adapter_config\": null,\n",
      "  \"add_adapter_cross_attn\": true,\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"add_layer_norm_after_adapter\": false,\n",
      "  \"add_layer_norm_before_adapter\": false,\n",
      "  \"additional_visual_embedding_layers\": 0,\n",
      "  \"answer_normalize\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"backbone\": \"facebook/bart-base\",\n",
      "  \"batch_size\": 128,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"caption_cocoonly\": true,\n",
      "  \"caption_only\": false,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier\": false,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"clip_grad_norm\": 5.0,\n",
      "  \"cls_task\": \"tinyimagenet\",\n",
      "  \"coco_only\": false,\n",
      "  \"comment\": \"\",\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_prompt_config\": null,\n",
      "  \"decoder_prompt_len\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"deepspeed\": null,\n",
      "  \"default_obj_order_ids\": [\n",
      "    50464,\n",
      "    50463,\n",
      "    50462,\n",
      "    50461,\n",
      "    50460,\n",
      "    50459,\n",
      "    50458,\n",
      "    50457,\n",
      "    50456,\n",
      "    50455,\n",
      "    50454,\n",
      "    50453,\n",
      "    50452,\n",
      "    50451,\n",
      "    50450,\n",
      "    50449,\n",
      "    50448,\n",
      "    50447,\n",
      "    50446,\n",
      "    50445,\n",
      "    50444,\n",
      "    50443,\n",
      "    50442,\n",
      "    50441,\n",
      "    50440,\n",
      "    50439,\n",
      "    50438,\n",
      "    50437,\n",
      "    50436,\n",
      "    50435,\n",
      "    50434,\n",
      "    50433,\n",
      "    50432,\n",
      "    50431,\n",
      "    50430,\n",
      "    50429,\n",
      "    50428,\n",
      "    50427,\n",
      "    50426,\n",
      "    50425,\n",
      "    50424,\n",
      "    50423,\n",
      "    50422,\n",
      "    50421,\n",
      "    50420,\n",
      "    50419,\n",
      "    50418,\n",
      "    50417,\n",
      "    50416,\n",
      "    50415,\n",
      "    50414,\n",
      "    50413,\n",
      "    50412,\n",
      "    50411,\n",
      "    50410,\n",
      "    50409,\n",
      "    50408,\n",
      "    50407,\n",
      "    50406,\n",
      "    50405,\n",
      "    50404,\n",
      "    50403,\n",
      "    50402,\n",
      "    50401,\n",
      "    50400,\n",
      "    50399,\n",
      "    50398,\n",
      "    50397,\n",
      "    50396,\n",
      "    50395,\n",
      "    50394,\n",
      "    50393,\n",
      "    50392,\n",
      "    50391,\n",
      "    50390,\n",
      "    50389,\n",
      "    50388,\n",
      "    50387,\n",
      "    50386,\n",
      "    50385,\n",
      "    50384,\n",
      "    50383,\n",
      "    50382,\n",
      "    50381,\n",
      "    50380,\n",
      "    50379,\n",
      "    50378,\n",
      "    50377,\n",
      "    50376,\n",
      "    50375,\n",
      "    50374,\n",
      "    50373,\n",
      "    50372,\n",
      "    50371,\n",
      "    50370,\n",
      "    50369,\n",
      "    50368,\n",
      "    50367,\n",
      "    50366,\n",
      "    50365\n",
      "  ],\n",
      "  \"distributed\": false,\n",
      "  \"do_lower_case\": false,\n",
      "  \"dora_simple\": false,\n",
      "  \"downsample\": true,\n",
      "  \"dropout\": 0.0,\n",
      "  \"dropout_rate\": 0.0,\n",
      "  \"dry\": false,\n",
      "  \"early_stopping\": true,\n",
      "  \"efficient_unique_hyper_net\": false,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"encoder_prompt_config\": null,\n",
      "  \"encoder_prompt_len\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"epochs\": 20,\n",
      "  \"expand_vis_embedding\": false,\n",
      "  \"factorized_phm\": true,\n",
      "  \"feat_dim\": 2048,\n",
      "  \"feature_type\": \"RN101\",\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"fp16\": false,\n",
      "  \"freeze_bn_statistics\": false,\n",
      "  \"freeze_ln_statistics\": false,\n",
      "  \"from_scratch\": false,\n",
      "  \"full_determinism\": false,\n",
      "  \"gen_max_length\": 20,\n",
      "  \"gpu\": 0,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"ground_upsample\": 1,\n",
      "  \"ground_weight\": 1,\n",
      "  \"hypercomplex_division\": 4,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"image_size\": \"(224,224)\",\n",
      "  \"individual_vis_layer_norm\": true,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_wandb\": false,\n",
      "  \"itm_cocoonly\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"lambda_z\": 0.001,\n",
      "  \"load\": null,\n",
      "  \"load_lxmert_qa\": null,\n",
      "  \"local_rank\": 0,\n",
      "  \"log_train_accuracy\": false,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dim\": 128,\n",
      "  \"lora_settings\": true,\n",
      "  \"losses\": \"lm,obj,attr,feat\",\n",
      "  \"low_rank_rank\": 1,\n",
      "  \"lr\": 0.01,\n",
      "  \"lr_scheduler_type\": \"linear\",\n",
      "  \"max_n_boxes\": 36,\n",
      "  \"max_n_eval_examples\": 2000,\n",
      "  \"max_n_train_examples\": 50000,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"max_text_length\": 20,\n",
      "  \"mid_dim\": 768,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"multiGPU\": true,\n",
      "  \"multitask_sampling\": \"roundrobin\",\n",
      "  \"n_boxes\": 36,\n",
      "  \"n_ground\": 1,\n",
      "  \"n_image_tokens\": 4,\n",
      "  \"n_images\": 2,\n",
      "  \"no_prefix\": false,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_workers\": 4,\n",
      "  \"obj_mask_rate\": 0.15,\n",
      "  \"oneddownsample\": false,\n",
      "  \"optim\": \"adamw\",\n",
      "  \"optimizer\": \"adamw\",\n",
      "  \"oscar_tags\": false,\n",
      "  \"output\": \"snap/VLBart_multitask/tune+lr1e-2_1e-2\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"phm_init_range\": 0.01,\n",
      "  \"phm_rank\": 1,\n",
      "  \"pos_dim\": 4,\n",
      "  \"position\": \"f11+l11\",\n",
      "  \"post_prompt\": \"\",\n",
      "  \"project_name\": \"RN101_LMsingle_dora_128_bs300_image224_lora_settings\",\n",
      "  \"projected_task_embedding_dim\": -1,\n",
      "  \"prompt\": \"vqa: \",\n",
      "  \"rank\": 1,\n",
      "  \"raw_label\": false,\n",
      "  \"reduction_factor\": 16,\n",
      "  \"remove_bn_vis_adapter\": false,\n",
      "  \"run_name\": \"tune+lr1e-2_plzplz2\",\n",
      "  \"scale_embedding\": false,\n",
      "  \"seed\": 9595,\n",
      "  \"share_down_sampler\": false,\n",
      "  \"share_up_sampler\": false,\n",
      "  \"share_vis_lang_layer_norm\": false,\n",
      "  \"share_weights\": true,\n",
      "  \"shared_phm_rule\": true,\n",
      "  \"shared_phm_rule_over_tasks\": false,\n",
      "  \"shuffle_boxes\": false,\n",
      "  \"single_vqa_prefix\": false,\n",
      "  \"sparse_sample\": false,\n",
      "  \"submit\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"tasks\": \"vqa\",\n",
      "  \"test\": null,\n",
      "  \"test_answerable\": false,\n",
      "  \"test_only\": false,\n",
      "  \"testing\": false,\n",
      "  \"tokenizer\": \"facebook/bart-base\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"track_z\": false,\n",
      "  \"train\": \"train\",\n",
      "  \"train_topk\": -1,\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"unfreeze_batch_norms\": false,\n",
      "  \"unfreeze_bias\": false,\n",
      "  \"unfreeze_decoder_layer_norms\": false,\n",
      "  \"unfreeze_encoder_layer_norms\": false,\n",
      "  \"unfreeze_language_model\": false,\n",
      "  \"unfreeze_layer_norms\": false,\n",
      "  \"unfreeze_lm_head\": false,\n",
      "  \"unfreeze_vis_encoder\": false,\n",
      "  \"unfreeze_vis_last_layer\": false,\n",
      "  \"unique_hyper_net\": false,\n",
      "  \"use_adam_for_visual\": false,\n",
      "  \"use_adapter\": false,\n",
      "  \"use_attn_prefix\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"use_compacter\": false,\n",
      "  \"use_data_augmentation\": false,\n",
      "  \"use_dora\": false,\n",
      "  \"use_hyperformer\": false,\n",
      "  \"use_lm_head_adapter\": false,\n",
      "  \"use_lora\": false,\n",
      "  \"use_lradapter\": false,\n",
      "  \"use_separate_optimizer_for_visual\": false,\n",
      "  \"use_single_adapter\": false,\n",
      "  \"use_single_lora\": false,\n",
      "  \"use_single_prompt\": false,\n",
      "  \"use_tasks_prompts\": true,\n",
      "  \"use_vis_adapter\": false,\n",
      "  \"use_vis_layer_norm\": true,\n",
      "  \"use_vis_order_embedding\": true,\n",
      "  \"use_vision\": true,\n",
      "  \"valid\": \"valid\",\n",
      "  \"valid_batch_size\": 128,\n",
      "  \"valid_topk\": -1,\n",
      "  \"vis_adapter_type\": \"middle-bottleneck\",\n",
      "  \"vis_lr\": 1e-05,\n",
      "  \"vis_pointer\": false,\n",
      "  \"vis_pooling_output\": false,\n",
      "  \"vis_reduction_factor\": 2,\n",
      "  \"vis_use_transformer\": false,\n",
      "  \"vis_weight_decay\": 0.01,\n",
      "  \"vocab_size\": 50465,\n",
      "  \"wandb_dir\": \"wandb\",\n",
      "  \"wandb_name\": \"peterwz\",\n",
      "  \"wandb_proj\": \"Reft\",\n",
      "  \"warmup_ratio\": 0.0,\n",
      "  \"weight_decay\": 0.005,\n",
      "  \"word_mask_rate\": 0.15,\n",
      "  \"world_size\": 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64daf75f-7f3e-432d-9e33-75ea90d4cd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method VQAFineTuneDataset.collate_fn of <vqa_clip_data.VQAFineTuneDataset object at 0x7f8899c1f220>>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2903c7a-0a60-40f5-9d9c-6509dc891d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['args', 'img_id', 'vis_feats', 'boxes', 'question_id', 'sent', 'input_ids', 'input_length', 'is_topk_optimal', 'label', 'answer', 'score', 'all_answers', 'target_ids', 'target_length'])\n",
      "['net']\n",
      "net\n"
     ]
    }
   ],
   "source": [
    "print(vqa_train_loader.dataset[0].keys())\n",
    "print(vqa_train_loader.dataset[0][\"all_answers\"])\n",
    "print(tokenizer.decode(vqa_train_loader.dataset[0][\"target_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae73144-b449-41a4-b7d1-1033117717f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForSeq2Seq\n",
    "# data_collator_fn = DataCollatorForSeq2Seq(\n",
    "#     tokenizer=tokenizer,\n",
    "#     model=model,\n",
    "#     label_pad_token_id=-100,\n",
    "#     padding=\"longest\"\n",
    "# )\n",
    "import transformers\n",
    "def keep_intervention_locations(datum):\n",
    "    new_data = {}\n",
    "    new_data[\"input_ids\"] = datum[\"input_ids\"]\n",
    "    # new_data[\"instruction\"] = datum[\"instruction\"]\n",
    "    new_data[\"intervention_locations\"] = datum[\"intervention_locations\"]\n",
    "    new_data[\"attention_mask\"] = datum[\"attention_mask\"]\n",
    "    # print(new_data[\"input_ids\"].shape, new_data[\"attention_mask\"])\n",
    "    return new_data\n",
    "\n",
    "def custom_collate_fn(data):\n",
    "    collate_fn_1 = train_dataset.collate_fn\n",
    "    collate_fn_2 = transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    # for item in data:\n",
    "    #     print(item[\"input_ids\"].shape)\n",
    "    output_1 = collate_fn_1(data)\n",
    "    custom_data = [keep_intervention_locations(item) for item in data]\n",
    "    output_2 = collate_fn_2(custom_data)\n",
    "    output = output_1\n",
    "    output[\"intervention_locations\"] = output_2[\"intervention_locations\"]\n",
    "    # print(output[\"intervention_locations\"].shape)\n",
    "    # print(torch.max(output[\"intervention_locations\"]))\n",
    "    # Offset image tokens' concatenation\n",
    "    # print(output[\"intervention_locations\"])\n",
    "    # output[\"intervention_locations\"][:,:,-last_pos:] += args.n_boxes\n",
    "    # output[\"intervention_locations\"] -= 1\n",
    "    # print(torch.max(output[\"intervention_locations\"]))\n",
    "    # print(output[\"intervention_locations\"])\n",
    "\n",
    "    # output[\"id\"] = output_2[\"id\"]\n",
    "    # output[\"labels\"] = output_2[\"labels\"]\n",
    "    \n",
    "    # output[\"attention_mask\"] = output_2[\"attention_mask\"]\n",
    "    # del output[\"attention_mask\"]\n",
    "\n",
    "    ids = []\n",
    "    instructions = []\n",
    "    for d in data:\n",
    "        ids.append(d[\"id\"])\n",
    "        instructions.append(d[\"instruction\"])\n",
    "    import numpy as np\n",
    "    output[\"id\"] = np.array(ids)\n",
    "    output[\"instruction\"] = instructions\n",
    "    \n",
    "    output[\"logits\"] = output[\"labels\"]\n",
    "    output[\"labels\"] = output[\"target_ids\"]\n",
    "    # output[\"instruction\"] = tokenizer.batch_decode(output[\"input_ids\"], skip_special_tokens=True)\n",
    "    # print(\"Output Keys:\", output.keys())\n",
    "    \n",
    "    # print(\"Input IDs:\", output[\"input_ids\"], tokenizer.batch_decode(output[\"input_ids\"], skip_special_tokens=True))\n",
    "    # print(\"Labels:\", output[\"labels\"].shape)\n",
    "    # labels = [[token for token in sequence if token != -100] for sequence in output[\"labels\"].tolist()]\n",
    "    # print(\"Labels:\", tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "    # print(\"Question IDs:\", output[\"question_ids\"])\n",
    "    # print(\"Answers:\", output[\"answers\"])\n",
    "    # print(\"All answers:\", output[\"all_answers\"])\n",
    "    # print(\"Scores:\", output[\"scores\"])\n",
    "\n",
    "    return output\n",
    "\n",
    "data_collator = ReftDataCollator(data_collator=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a497868a-120f-4abe-8bc7-bc1b9cbb264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = args.rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "668e60d3-61d0-48ca-bf94-b787879b1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [{\n",
    "    \"layer\": l, \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": rank,\n",
    "    \"intervention\": LoreftIntervention(\n",
    "        embed_dim=model.config.d_model, low_rank_dimension=rank,\n",
    "        dropout=args.dropout, dtype=torch.float32, act_fn=None, device=\"cuda\",\n",
    "        add_bias=True\n",
    "    )\n",
    "} for l in layers]\n",
    "task_type=TaskType.CAUSAL_LM\n",
    "\n",
    "reft_config = ReftConfig(representations=representations)\n",
    "empty_reft_config = ReftConfig(representations=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0271831c-7645-4ca5-845f-7b5a4caf8fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 0 || trainable model params: 0\n",
      "model params: 141,156,864 || trainable%: 0.0\n",
      "trainable intervention params: 9,222 || trainable model params: 0\n",
      "model params: 141,156,864 || trainable%: 0.006533157324889281\n"
     ]
    }
   ],
   "source": [
    "reft_model = get_reft_model(model, reft_config)\n",
    "empty_reft_model = get_reft_model(model, empty_reft_config)\n",
    "empty_reft_model.print_trainable_parameters()\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9673c723-0a55-4b39-b261-b0bced33e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"random\",\n",
    "    run_name=\"random\",\n",
    "    num_train_epochs=args.epochs,\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    "    per_device_eval_batch_size=args.batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"no\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    metric_for_best_model=None,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1, # for GLUE, it will save 2 at max.\n",
    "    logging_steps=1,\n",
    "    learning_rate=args.lr,\n",
    "    # learning_rate=1e-4,\n",
    "    warmup_ratio=args.warmup_ratio,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=args.weight_decay,\n",
    "    # lr_scehuler=\"none\",\n",
    "    lr_scheduler_type=args.lr_scheduler_type,\n",
    "    report_to=\"wandb\" if args.is_wandb else \"none\",\n",
    "    use_cpu=False,\n",
    "    seed=42,\n",
    "    # until HF supports ReFT, this remains False! :)\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fce2fca-1c68-40bd-b143-a0d5267bb3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvene import IntervenableModel\n",
    "# from overrides import overrides\n",
    "\n",
    "class MyTrainer(ReftTrainerForCausalLM):\n",
    "    # @overrides\n",
    "    def training_step(self, model, batch):\n",
    "        # print(\"My trainer step\")\n",
    "        batch = self._prepare_inputs(batch)\n",
    "\n",
    "        # print(\"Batch:\", batch.keys())\n",
    "        device = batch['input_ids'].device\n",
    "\n",
    "        batch = model.model.vis_forward(batch, device)\n",
    "        task = batch[\"task\"]\n",
    "\n",
    "        vis_feats = batch['vis_feats']\n",
    "        input_ids = batch['input_ids']\n",
    "        vis_pos = batch['boxes']\n",
    "\n",
    "        lm_labels = batch[\"target_ids\"].to(device)\n",
    "\n",
    "        inputs = {**batch}\n",
    "        inputs[\"return_dict\"] = True\n",
    "        inputs[\"reduce_loss\"] = False\n",
    "        inputs[\"vis_inputs\"] = (vis_feats, vis_pos)\n",
    "        # print(inputs.keys())\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            self.accelerator.backward(loss)\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        intervenable: IntervenableModel,\n",
    "        inputs,\n",
    "        return_outputs=False\n",
    "    ):\n",
    "        \n",
    "        lm_labels = inputs[\"target_ids\"]\n",
    "        # print(\"KEYS:\", inputs.keys())\n",
    "        # print(\"LABELS:\", lm_labels)\n",
    "        # print(\"SCORES:\", inputs[\"scores\"])\n",
    "        # print(\"LOCS:\", inputs[\"intervention_locations\"])\n",
    "        # print(\"INPUT_IDS:\", inputs[\"input_ids\"])\n",
    "        # print(\"VIS_INPUTS:\", inputs[\"vis_inputs\"][0].shape, inputs[\"vis_inputs\"][1].shape)\n",
    "        \n",
    "        _, cf_outputs = intervenable(\n",
    "            {\n",
    "                \"input_ids\": inputs[\"input_ids\"],\n",
    "                # \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                \"vis_inputs\": inputs[\"vis_inputs\"],\n",
    "                \"task\": \"vqa\",\n",
    "                \n",
    "            },\n",
    "            unit_locations={\"sources->base\": (\n",
    "                None,\n",
    "                inputs[\"intervention_locations\"].permute(1, 0, 2).tolist()\n",
    "            )},\n",
    "            labels=inputs[\"target_ids\"],\n",
    "            subspaces=None,\n",
    "        )\n",
    "        # return\n",
    "        loss = cf_outputs.loss\n",
    "        # print(\"CF OUTPUTS:\", cf_outputs.keys(), len(cf_outputs[\"loss\"]))\n",
    "        \n",
    "        \n",
    "        lm_mask = (lm_labels != -100).float()\n",
    "        # print(\"LM MASK:\", lm_mask)\n",
    "        # print(\"SCORES:\", inputs[\"scores\"])\n",
    "        B, L = lm_labels.size()\n",
    "\n",
    "        loss = loss.view(B, L) * lm_mask\n",
    "\n",
    "        loss = loss.sum(dim=1) / lm_mask.sum(dim=1).clamp(min=1)  # B\n",
    "\n",
    "        loss = loss * inputs[\"scores\"]\n",
    "\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5201485-07d7-4dfc-b8ac-b64c1edeffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = MyTrainer(\n",
    "    model=reft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d479808-d1a7-4343-8307-48a65eb0ac15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLBartMultiTask(\n",
      "  (model): VLBartModel(\n",
      "    (shared): Embedding(50465, 768)\n",
      "    (encoder): JointEncoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (visual_embedding): VisualEmbedding(\n",
      "        (feat_embedding): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (absolute_vis_pos_embedding): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (obj_order_embedding): Embedding(50465, 768)\n",
      "        (img_order_embedding): Embedding(2, 768)\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "      )\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50465, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(reft_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "821cce64-1027-4356-b66b-981a570a004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.is_wandb:\n",
    "    import wandb\n",
    "    run = wandb.init(\n",
    "        project=f\"{args.wandb_proj}_vqa\", \n",
    "        entity=args.wandb_name,\n",
    "        name=args.run_name,\n",
    "        dir=args.wandb_dir,\n",
    "    )\n",
    "    run.summary.update(vars(args))\n",
    "    n_params = reft_model.count_parameters(include_model=False)\n",
    "    wandb.log(\n",
    "        {\"train/n_params\": n_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e46e94b8-3d45-430d-91bd-10ef4484967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9145, 'learning_rate': 0.0095, 'epoch': 1.0}\n",
      "{'loss': 0.8003, 'learning_rate': 0.009000000000000001, 'epoch': 2.0}\n",
      "{'loss': 0.791, 'learning_rate': 0.0085, 'epoch': 3.0}\n",
      "{'loss': 0.7841, 'learning_rate': 0.008, 'epoch': 4.0}\n",
      "{'loss': 0.7779, 'learning_rate': 0.0075, 'epoch': 5.0}\n",
      "{'loss': 0.7756, 'learning_rate': 0.006999999999999999, 'epoch': 6.0}\n",
      "{'loss': 0.7706, 'learning_rate': 0.006500000000000001, 'epoch': 7.0}\n",
      "{'loss': 0.7655, 'learning_rate': 0.006, 'epoch': 8.0}\n",
      "{'loss': 0.7598, 'learning_rate': 0.0055000000000000005, 'epoch': 9.0}\n",
      "{'loss': 0.7573, 'learning_rate': 0.005, 'epoch': 10.0}\n",
      "{'loss': 0.7507, 'learning_rate': 0.0045000000000000005, 'epoch': 11.0}\n",
      "{'loss': 0.7453, 'learning_rate': 0.004, 'epoch': 12.0}\n",
      "{'loss': 0.7405, 'learning_rate': 0.0034999999999999996, 'epoch': 13.0}\n",
      "{'loss': 0.7329, 'learning_rate': 0.003, 'epoch': 14.0}\n",
      "{'loss': 0.7273, 'learning_rate': 0.0025, 'epoch': 15.0}\n",
      "{'loss': 0.7196, 'learning_rate': 0.002, 'epoch': 16.0}\n",
      "{'loss': 0.713, 'learning_rate': 0.0015, 'epoch': 17.0}\n",
      "{'loss': 0.7052, 'learning_rate': 0.001, 'epoch': 18.0}\n",
      "{'loss': 0.6974, 'learning_rate': 0.0005, 'epoch': 19.0}\n",
      "{'loss': 0.6892, 'learning_rate': 0.0, 'epoch': 20.0}\n",
      "{'train_runtime': 2498.3883, 'train_samples_per_second': 400.258, 'train_steps_per_second': 3.13, 'train_loss': 0.7558738923133792, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7820, training_loss=0.7558738923133792, metrics={'train_runtime': 2498.3883, 'train_samples_per_second': 400.258, 'train_steps_per_second': 3.13, 'train_loss': 0.7558738923133792, 'epoch': 20.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b39f7722-e83d-4543-811e-faa6345179b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(\"tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "943603b8-ecdc-43aa-8c69-546bed34016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyreft\n",
    "# reft_model = pyreft.ReftModel.load(\n",
    "#     \"temp-outputs\", model\n",
    "# )\n",
    "# reft_model.set_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1bf162a-1788-48dc-9ac9-bd2bb76cb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "reft_model.model.eval()\n",
    "for k,v in reft_model.interventions.items():\n",
    "    _ = v[0].eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "835e19b8-bb8b-456e-b05c-b3e0ad97bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                            | 0/2000 [00:00<?, ?it/s]/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  5%|█████▍                                                                                                  | 104/2000 [00:04<01:35, 19.90it/s, em=0.279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: What is this piece of furniture used for?  |  wood  |  sitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████▌                                                                                              | 202/2000 [00:10<01:41, 17.78it/s, em=0.27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: How many orange cones are there?  |  1  |  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████████▊                                                                                        | 303/2000 [00:15<01:34, 17.91it/s, em=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: Has the skier fallen?  |  no  |  yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████▉                                                                                   | 403/2000 [00:20<01:06, 24.07it/s, em=0.267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: What color is the bed sheets?  |  white  |  blue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████████▎                                                                             | 505/2000 [00:24<01:04, 23.09it/s, em=0.281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: What is she wearing on her head?  |  scarf  |  hat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████▋                                                   | 1003/2000 [00:46<00:39, 25.01it/s, em=0.294]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: The crosswalk sign is indicating what?  |  stop  |  walk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████████████████████████████████████▉                                              | 1105/2000 [00:51<00:40, 22.32it/s, em=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: How many windows are in the right side of the plane?  |  2  |  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████▊                                         | 1201/2000 [00:56<00:35, 22.41it/s, em=0.281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: What is behind the bike?  |  sand  |  bag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████████████████████▎                              | 1403/2000 [01:05<00:24, 24.32it/s, em=0.288]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: What is in the pregnant woman's belly?  |  nothing  |  baby\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████████████████████▌                         | 1505/2000 [01:09<00:20, 23.87it/s, em=0.282]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: How many kids are holding game controllers?  |  1  |  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████▌                    | 1603/2000 [01:14<00:20, 19.18it/s, em=0.281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: What condition is the water in?  |  snow  |  wavy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████████████████████████████████████████████████████████▊               | 1704/2000 [01:20<00:14, 20.54it/s, em=0.282]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: Can you see the hook up for the train?  |  no  |  yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████▊          | 1803/2000 [01:25<00:13, 14.66it/s, em=0.278]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: What are the bears sitting on?  |  tree  |  cart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████     | 1903/2000 [01:32<00:05, 17.31it/s, em=0.277]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa: How many colors are represented in this scene?  |  2  |  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [01:38<00:00, 20.34it/s, em=0.279]\n"
     ]
    }
   ],
   "source": [
    "from compute_metrics import compute_metrics\n",
    "generations, stats = compute_metrics(\n",
    "    \"vqa\", \"vqa\", reft_model, tokenizer, eval_dataset, eval_dataset,\n",
    "    '', 'test', 1, # batch_size\n",
    "    data_collator,\n",
    "    split=False, greedy_decoding=True, temperature=1.0, top_p=None, top_k=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b22b2551-67c7-4603-a91e-43f9e555d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset[3][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0cbfba6-7c53-431e-a2de-c27a0d23cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generations[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80133794-e079-4c06-8de3-c8a6a076db68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval/vqa': 0.2785}\n"
     ]
    }
   ],
   "source": [
    "if args.is_wandb:\n",
    "    wandb.log(stats)\n",
    "else:\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a539b17-812f-4144-90a5-d5d9e6be99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reft_model.save('temp-outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5b870-29e4-4fd5-9f23-429366ed606a",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "\n",
    "1. Speed up data loading [open ended perf problem]\n",
    "2. Checkup the intervention locations for VL-BART\n",
    "3. Fine-tuned model's performance on eval/test VQA\n",
    "4. Fine-tuned model manual validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20d672-63e3-45d1-81c0-9f6540ce6f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Jailbreak with ReFT

> [!Note]  
> We've decided not to publish under this folder at this point. Previous files still exist in the git history - you can get the vibe there, but it's not what we want to pursue.

> [!WARNING]  
> This study includes content that might enable individuals to create damaging material using certain public large language models (LLMs). Although there are inherent risks, we think it's crucial to fully reveal this research. We insist ReFT should only be used to study model's safety features, and must not use ReFT to distribute models that can generate harmful content at scale. We believe ReFT would eventually enhance model safety.
